{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6086245e-a7ac-474b-a9a9-ee3b7227f8e8",
   "metadata": {},
   "source": [
    "for use with NDVI product from LTDR raw dataset\n",
    "\n",
    "- Prepares list of all files\n",
    "- Builds list of day files to process\n",
    "- Processes day files\n",
    "- Builds list of day files to aggregate to months\n",
    "- Run month aggregation\n",
    "- Builds list of month files to aggregate to years\n",
    "- Run year aggregation\n",
    "\n",
    "example LTDR product file names (ndvi product code is AVH13C1)\n",
    "\n",
    "AVH13C1.A1981181.N07.004.2013227210959.hdf\n",
    "\n",
    "split file name by \".\"\n",
    "eg:\n",
    "\n",
    "full file name - \"AVH13C1.A1981181.N07.004.2013227210959.hdf\"\n",
    "\n",
    "0     product code        AVH13C1\n",
    "1     date of image       A1981181\n",
    "2     sensor code         N07\n",
    "3     misc                004\n",
    "4     processed date      2013227210959\n",
    "5     extension           hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac5dadf0-45bc-41c0-a519-15e11de41108",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import ssl\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from collections import OrderedDict\n",
    "from configparser import ConfigParser\n",
    "from typing import Any, Generator, List, Literal, Tuple, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a8354ea0-2477-4aee-a100-894d8299f1b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.crs import CRS\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0fe710-4930-4520-a174-96086fbaaa73",
   "metadata": {},
   "source": [
    "TODO: check on line below, should have been set up by dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c612f260-bd09-4c84-b05d-1cf100707a6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 23.7.4\n",
      "  latest version: 24.1.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "Or to minimize the number of packages updated during conda update use\n",
      "\n",
      "     conda install conda=24.1.0\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge gdal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c366db85-a1e1-46f8-8944-98dc01d02f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal, osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a38bee93-9911-4b5b-b75b-f2a03cad579e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d0231105-0cbf-4118-9287-a4a2ae433043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_config_dict(config_file=\"config.ini\"):\n",
    "    config = ConfigParser()\n",
    "    config.read(config_file)\n",
    "    token = config[\"main\"][\"token\"]\n",
    "\n",
    "    return {\n",
    "        token: config[\"main\"][\"token\"],\n",
    "        \"years\": [int(y) for y in config[\"main\"][\"years\"].split(\", \")],\n",
    "        \"raw_dir\": Path(config[\"main\"][\"raw_dir\"]),\n",
    "        \"output_dir\": Path(config[\"main\"][\"output_dir\"]),\n",
    "        \"overwrite_download\": config[\"main\"].getboolean(\"overwrite_download\"),\n",
    "        \"validate_download\": config[\"main\"].getboolean(\"validate_download\"),\n",
    "        \"overwrite_processing\": config[\"main\"].getboolean(\"overwrite_processing\"),\n",
    "        \"backend\": config[\"run\"][\"backend\"],\n",
    "        \"task_runner\": config[\"run\"][\"task_runner\"],\n",
    "        \"run_parallel\": config[\"run\"].getboolean(\"run_parallel\"),\n",
    "        \"max_workers\": int(config[\"run\"][\"max_workers\"]),\n",
    "        \"log_dir\": Path(config[\"main\"][\"raw_dir\"]) / \"logs\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7198632c-18ca-486f-9e2a-2b51b46d81fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = ConfigParser()\n",
    "config.read(\"config.ini\")\n",
    "token = config[\"main\"][\"token\"]\n",
    "years = [int(y) for y in config[\"main\"][\"years\"].split(\", \")]\n",
    "raw_dir = Path(config[\"main\"][\"raw_dir\"])\n",
    "output_dir = Path(config[\"main\"][\"output_dir\"])\n",
    "overwrite_download = config[\"main\"].getboolean(\"overwrite_download\")\n",
    "validate_download = config[\"main\"].getboolean(\"validate_download\")\n",
    "overwrite_processing = config[\"main\"].getboolean(\"overwrite_processing\")\n",
    "backend = config[\"run\"][\"backend\"]\n",
    "task_runner = config[\"run\"][\"task_runner\"]\n",
    "run_parallel = config[\"run\"].getboolean(\"run_parallel\")\n",
    "max_workers = int(config[\"run\"][\"max_workers\"])\n",
    "log_dir = Path(config[\"main\"][\"raw_dir\"]) / \"logs\"\n",
    "build_list = [\"daily\", \"monthly\",\"yearly\"]\n",
    "auth_headers = { \"Authorization\": f\"Bearer {token}\" }\n",
    "name = \"Long-term Data Record NDVI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d21bc666-380a-44d0-b493-10aac1f617c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " def get_logger():\n",
    "        \"\"\"\n",
    "        This function will return a logger that implements the Python logging API:\n",
    "        https://docs.python.org/3/library/logging.html\n",
    "\n",
    "        If you are using Prefect, the logs will be managed by Prefect\n",
    "        \"\"\"\n",
    "        if backend == \"prefect\":\n",
    "            from prefect import get_run_logger\n",
    "            return get_run_logger()\n",
    "        else:\n",
    "            return logging.getLogger(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b43d8adc-e0e7-440c-8848-c2e61b1f5f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_url = \"https://ladsweb.modaps.eosdis.nasa.gov/api/v2/content/details/allData/465/\"\n",
    "sensors = [\n",
    "            \"N07_AVH13C1\",\n",
    "            \"N09_AVH13C1\",\n",
    "            \"N11_AVH13C1\",\n",
    "            \"N14_AVH13C1\",\n",
    "            \"N16_AVH13C1\",\n",
    "            \"N18_AVH13C1\",\n",
    "            \"N19_AVH13C1\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c4e066f-c1ae-4d0b-ad1d-5e96712ea3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_sensor_download_list(sensor: str):\n",
    "        logger = get_logger()\n",
    "\n",
    "        # generates dictionaries that represent each entry in a directory\n",
    "        def dir_contents(dir_url: str) -> List[dict]:\n",
    "            logger.debug(f\"Fetching {dir_url}\")\n",
    "            description: dict = json.loads(requests.get(dir_url).content)\n",
    "            return description[\"content\"]\n",
    "\n",
    "        # validates md5 hash of a file\n",
    "        def validate(filepath: Union[str, os.PathLike], md5: str) -> bool:\n",
    "            with open(filepath, \"rb\") as chk:\n",
    "                data = chk.read()\n",
    "                return md5 == hashlib.md5(data).hexdigest()\n",
    "\n",
    "        # this is what we'll return\n",
    "        # list of tuples, each including:\n",
    "        #   1. a boolean \"does the file need to be downloaded?\"\n",
    "        #   2. another tuple: (url_of_download, dst_path_of_download)\n",
    "        download_list: List[Tuple[bool, Tuple[str, Type[Path]]]] = []\n",
    "\n",
    "        sensor_dir: str = urljoin(dataset_url, sensor)\n",
    "        # for each year the sensor collected data\n",
    "        for year_details in dir_contents(sensor_dir):\n",
    "            # is this a year we'd like data from?\n",
    "            if int(year_details[\"name\"]) in years:\n",
    "                year_dir: str = \"/\".join([sensor_dir, year_details[\"name\"]])\n",
    "                # for each day the sensor collected data in this year\n",
    "                for day_details in dir_contents(year_dir):\n",
    "                    day_dir: str = \"/\".join([year_dir, day_details[\"name\"]])\n",
    "                    # for each file the sensor created for this day\n",
    "                    for file_detail in dir_contents(day_dir):\n",
    "                        day_download_url: str = file_detail[\"downloadsLink\"]\n",
    "                        dst = raw_dir / sensor / year_details[\"name\"] / day_details[\"name\"] / file_detail[\"name\"]\n",
    "                        # if file is already downloaded, and we aren't in overwrite mode\n",
    "                        if dst.exists() and not overwrite_download:\n",
    "                            if validate_download:\n",
    "                                if validate(dst, file_detail[\"md5sum\"]):\n",
    "                                    logger.info(f\"File validated: {dst.as_posix()}\")\n",
    "                                    download_list.append((False, (day_download_url, dst)))\n",
    "                                else:\n",
    "                                    logger.info(f\"File validation failed, queuing for download: {dst.as_posix()}\")\n",
    "                                    download_list.append((True, (day_download_url, dst)))\n",
    "                            else:\n",
    "                                logger.info(f\"File exists, skipping: {dst.as_posix()}\")\n",
    "                                download_list.append((False, (day_download_url, dst)))\n",
    "                        else:\n",
    "                            logger.info(f\"Queuing for download: {day_download_url}\")\n",
    "                            download_list.append((True, (day_download_url, dst)))\n",
    "        return download_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7bcd4605-7c57-43a1-8056-2b718f5fac75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tmp_to_dst_file(final_dst, tmp_dir=None):\n",
    "        logger = get_logger()\n",
    "        with TemporaryDirectory(dir=tmp_dir) as tmp_sub_dir:\n",
    "            tmp_file = mkstemp(dir=tmp_sub_dir)[1]\n",
    "            logger.debug(f\"Created temporary file {tmp_file} with final destination {str(final_dst)}\")\n",
    "            yield tmp_file\n",
    "            try:\n",
    "                shutil.move(tmp_file, final_dst)\n",
    "            except:\n",
    "                logger.exception(f\"Failed to transfer temporary file {tmp_file} to final destination {str(final_dst)}\")\n",
    "            else:\n",
    "                logger.debug(f\"Successfully transferred {tmp_file} to final destination {str(final_dst)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c32b7d5-f882-4235-af0c-8671cc670503",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def download(src_url: str, final_dst_path: Union[str, os.PathLike]) -> None:\n",
    "        logger = dataset.get_logger()\n",
    "        logger.info(f\"Downloading {str(final_dst_path)}...\")\n",
    "        with requests.get(src_url, headers=auth_headers, stream=True) as src:\n",
    "            src.raise_for_status()\n",
    "            with tmp_to_dst_file(final_dst_path) as dst_path:\n",
    "                with open(dst_path, \"wb\") as dst:\n",
    "                    for chunk in src.iter_content(chunk_size=8192):\n",
    "                        dst.write(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ee06b0a-d775-4884-b9b0-5516cb96a342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_process_list(downloaded_files):\n",
    "\n",
    "        # filter options to accept/deny based on sensor, year\n",
    "        # all values must be strings\n",
    "        # do not enable/use both accept/deny for a given field\n",
    "\n",
    "        ops = {\n",
    "            \"use_sensor_accept\": False,\n",
    "            \"sensor_accept\": [],\n",
    "            \"use_sensor_deny\": False,\n",
    "            \"sensor_deny\": [],\n",
    "            \"use_year_accept\": True,\n",
    "            \"year_accept\": [\"2019\", \"2020\"],\n",
    "            \"use_year_deny\": False,\n",
    "            \"year_deny\": [\"2019\"]\n",
    "        }\n",
    "\n",
    "        df_dict_list = []\n",
    "\n",
    "        for input_path in downloaded_files:\n",
    "            items = input_path.stem.split(\".\")\n",
    "            year = items[1][1:5]\n",
    "            day = items[1][5:8]\n",
    "            sensor = items[2]\n",
    "            month = \"{0:02d}\".format(datetime.strptime(f\"{year}+{day}\", \"%Y+%j\").month)\n",
    "            output_path = output_dir / \"daily\" / f\"avhrr_ndvi_v5_{sensor}_{year}_{day}.tif\"\n",
    "            df_dict_list.append({\n",
    "                \"input_path\": input_path,\n",
    "                \"sensor\": sensor,\n",
    "                \"year\": year,\n",
    "                \"month\": month,\n",
    "                \"day\": day,\n",
    "                \"year_month\": year+\"_\"+month,\n",
    "                \"year_day\": year+\"_\"+day,\n",
    "                \"output_path\": output_path\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(df_dict_list).sort_values(by=[\"input_path\"])\n",
    "\n",
    "        # df = df.drop_duplicates(subset=\"year_day\", take_last=True)\n",
    "        sensors = sorted(list(set(df[\"sensor\"])))\n",
    "        years = sorted(list(set(df[\"year\"])))\n",
    "        filter_sensors = None\n",
    "        if ops['use_sensor_accept']:\n",
    "            filter_sensors = [i for i in sensors if i in ops['sensor_accept']]\n",
    "        elif ops['use_sensor_deny']:\n",
    "            filter_sensors = [i for i in sensors if i not in ops['sensor_deny']]\n",
    "        if filter_sensors:\n",
    "            df = df.loc[df[\"sensor\"].isin(filter_sensors)]\n",
    "        filter_years = None\n",
    "        if ops['use_year_accept']:\n",
    "            filter_years = [i for i in years if i in ops['year_accept']]\n",
    "        elif ops['use_year_deny']:\n",
    "            filter_years = [i for i in years if i not in ops['year_deny']]\n",
    "        if filter_years:\n",
    "            df = df.loc[df[\"year\"].isin(filter_years)]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "84af5ef6-a8b1-4e68-a879-1496c26af851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_mask(qa_array, mask_vals):\n",
    "        qa_mask_vals = [abs(x - 15) for x in mask_vals]\n",
    "        mask_bin_array = [0] * 16\n",
    "        for x in qa_mask_vals:\n",
    "            mask_bin_array[x] = 1\n",
    "        mask_bin = int(\"\".join(map(str, mask_bin_array)), 2)\n",
    "\n",
    "        flag = lambda i: (i & 65535 & mask_bin) != 0\n",
    "\n",
    "        qa_mask = pd.DataFrame(qa_array).applymap(flag).to_numpy()\n",
    "        return qa_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d23359d-35d7-443e-9618-01b77af75b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_daily_data(src, output_path):\n",
    "            \"\"\"\n",
    "            Process input raster and create output in output directory\n",
    "\n",
    "            Unpack NDVI subdataset from a HDF container\n",
    "            Reproject to EPSG:4326\n",
    "            Set values <0 (other than nodata) to 0\n",
    "            Write to COG\n",
    "\n",
    "            Parts of code pulled from:\n",
    "\n",
    "            https://gis.stackexchange.com/questions/174017/extract-scientific-layers-from-modis-hdf-dataeset-using-python-gdal\n",
    "            https://gis.stackexchange.com/questions/42584/how-to-call-gdal-translate-from-python-code\n",
    "            https://stackoverflow.com/questions/10454316/how-to-project-and-resample-a-grid-to-match-another-grid-with-gdal-python/10538634#10538634\n",
    "            https://jgomezdans.github.io/gdal_notes/reprojection.html\n",
    "\n",
    "            Notes:\n",
    "\n",
    "            Rebuilding geotransform is not really necessary in this case but might\n",
    "            be useful for future data prep scripts that can use this as startng point.\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            logger = get_logger()\n",
    "\n",
    "            year = src.name.split(\".\")[1][1:5]\n",
    "            day = src.name.split(\".\")[1][5:8]\n",
    "            sensor = src.name.split(\".\")[2]\n",
    "\n",
    "            if output_path.exists() and not overwrite_processing:\n",
    "                logger.info(f\"Skipping day, already processed: {sensor} {year} {day}\")\n",
    "            else:\n",
    "                logger.info(f\"Processing day: {sensor} {year} {day}\")\n",
    "\n",
    "                # list of qa fields and bit numbers\n",
    "                # https://ltdr.modaps.eosdis.nasa.gov/ltdr/docs/AVHRR_LTDR_V5_Document.pdf\n",
    "\n",
    "                qa_bits = {\n",
    "                    15: \"Polar flag: latitude > 60deg (land) or > 50deg (ocean)\",\n",
    "                    14: \"BRDF-correction issues\",\n",
    "                    13: \"RHO3 value is invalid\",\n",
    "                    12: \"Channel 5 value is invalid\",\n",
    "                    11: \"Channel 4 value is invalid\",\n",
    "                    10: \"Channel 3 value is invalid\",\n",
    "                    9: \"Channel 2 (NIR) value is invalid\",\n",
    "                    8: \"Channel 1 (visible) value is invalid\",\n",
    "                    7: \"Channel 1-5 are invalid\",\n",
    "                    6: \"Pixel is at night (high solar zenith angle)\",\n",
    "                    5: \"Pixel is over dense dark vegetation\",\n",
    "                    4: \"Pixel is over sun glint\",\n",
    "                    3: \"Pixel is over water\",\n",
    "                    2: \"Pixel contains cloud shadow\",\n",
    "                    1: \"Pixel is cloudy\",\n",
    "                    0: \"Unused\"\n",
    "                }\n",
    "\n",
    "                # qa_mask_vals = [15, 9, 8, 6, 4, 3, 2, 1]\n",
    "                qa_mask_vals = [15, 9, 8, 1]\n",
    "\n",
    "                ndvi_gdal_path = f\"HDF4_EOS:EOS_GRID:\\\"{src.as_posix()}\\\":Grid:NDVI\"\n",
    "                qa_gdal_path = f\"HDF4_EOS:EOS_GRID:\\\"{src.as_posix()}\\\":Grid:QA\"\n",
    "\n",
    "                # open data subdataset\n",
    "                with rasterio.open(ndvi_gdal_path) as ndvi_src:\n",
    "                    ndvi_array = ndvi_src.read(1)\n",
    "\n",
    "                    # open quality assurance subdataset\n",
    "                    with rasterio.open(qa_gdal_path) as qa_src:\n",
    "                        qa_array = qa_src.read(1)\n",
    "\n",
    "                        # create mask array using our chosen mask values\n",
    "                        qa_mask = create_mask(qa_array, qa_mask_vals)\n",
    "\n",
    "                        # apply mask to dataset\n",
    "                        ndvi_array[qa_mask] = -9999\n",
    "\n",
    "                    ndvi_array[np.where((ndvi_array < 0) & (ndvi_array > -9999))] = 0\n",
    "                    ndvi_array[np.where(ndvi_array > 10000)] = 10000\n",
    "\n",
    "                    profile = {\n",
    "                        \"count\": 1,\n",
    "                        \"driver\": \"COG\",\n",
    "                        \"compress\": \"LZW\",\n",
    "                        \"dtype\": \"int16\",\n",
    "                        \"nodata\": -9999,\n",
    "                        \"height\": 3600,\n",
    "                        \"width\": 7200,\n",
    "                        \"crs\": CRS.from_epsg(4326),\n",
    "                        \"transform\": ndvi_src.transform,\n",
    "                    }\n",
    "\n",
    "                    with tmp_to_dst_file(output_path) as dst_path:\n",
    "                        with rasterio.open(dst_path, \"w\", **profile) as dst:\n",
    "                            # for some reason rasterio raises an exception if we don't specify that there is one index\n",
    "                            dst.write(ndvi_array, indexes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cd8f4ea3-b468-4cce-83c1-cea588a2f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_monthly_data(year_month, month_files, month_path):\n",
    "            logger = get_logger()\n",
    "            if os.path.exists(month_path) and not overwrite_processing:\n",
    "                logger.info(f\"Skipping month, already processed: {year_month}\")\n",
    "            else:\n",
    "                logger.info(f\"Processing month: {year_month}\")\n",
    "                data, meta = aggregate_rasters(file_list=month_files, method=\"max\")\n",
    "                write_raster(month_path, data, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "431dc11e-296c-4416-bcd5-3d03916d44ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def aggregate_rasters(file_list, method=\"mean\"):\n",
    "            \"\"\"\n",
    "            Aggregate multiple rasters\n",
    "\n",
    "            Aggregates multiple rasters with same features (dimensions, transform,\n",
    "            pixel size, etc.) and creates single layer using aggregation method\n",
    "            specified.\n",
    "\n",
    "            Supported methods: mean (default), max, min, sum\n",
    "\n",
    "            Arguments\n",
    "                file_list (list): list of file paths for rasters to be aggregated\n",
    "                method (str): method used for aggregation\n",
    "\n",
    "            Return\n",
    "                result: rasterio Raster instance\n",
    "            \"\"\"\n",
    "            logger = get_logger()\n",
    "            store = None\n",
    "            for ix, file_path in enumerate(file_list):\n",
    "\n",
    "                try:\n",
    "                    raster = rasterio.open(file_path)\n",
    "                except:\n",
    "                    logger.error(f\"Could not include file in aggregation ({str(file_path)})\")\n",
    "                    continue\n",
    "\n",
    "                active = raster.read(masked=True)\n",
    "\n",
    "                if store is None:\n",
    "                    store = active.copy()\n",
    "\n",
    "                else:\n",
    "                    # make sure dimensions match\n",
    "                    if active.shape != store.shape:\n",
    "                        raise Exception(\"Dimensions of rasters do not match\")\n",
    "\n",
    "                    if method == \"max\":\n",
    "                        store = np.ma.array((store, active)).max(axis=0)\n",
    "\n",
    "                        # non masked array alternatives\n",
    "                        # store = np.maximum.reduce([store, active])\n",
    "                        # store = np.vstack([store, active]).max(axis=0)\n",
    "\n",
    "                    elif method == \"mean\":\n",
    "                        if ix == 1:\n",
    "                            weights = (~store.mask).astype(int)\n",
    "\n",
    "                        store = np.ma.average(np.ma.array((store, active)), axis=0, weights=[weights, (~active.mask).astype(int)])\n",
    "                        weights += (~active.mask).astype(int)\n",
    "\n",
    "                    elif method == \"min\":\n",
    "                        store = np.ma.array((store, active)).min(axis=0)\n",
    "\n",
    "                    elif method == \"sum\":\n",
    "                        store = np.ma.array((store, active)).sum(axis=0)\n",
    "\n",
    "                    else:\n",
    "                        raise Exception(\"Invalid method\")\n",
    "\n",
    "            store = store.filled(raster.nodata)\n",
    "            return store, raster.profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9b7ba13d-4edf-472e-8bdb-04133473a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_raster(path, data, meta):\n",
    "        logger = get_logger()\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        meta[\"dtype\"] = data.dtype\n",
    "        with tmp_to_dst_file(path) as write_path:\n",
    "            with rasterio.open(write_path, \"w\", **meta) as result:\n",
    "                try:\n",
    "                    result.write(data)\n",
    "                except:\n",
    "                    logger.exception(\"Error writing raster to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f32b4ff-d6b4-4484-877d-2f639d373133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_yearly_data(year, year_files, year_path):\n",
    "        logger = get_logger()\n",
    "        if os.path.exists(year_path) and not overwrite_processing:\n",
    "            logger.info(f\"Skipping year, already processed: {year}\")\n",
    "        else:\n",
    "            logger.info(f\"Processing year: {year}\")\n",
    "            data, meta = aggregate_rasters(file_list=year_files, method=\"mean\")\n",
    "            write_raster(year_path, data, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "348bebfb-7716-4bd4-a65a-f06b0b191f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_retries(cur_retries: int, cur_retry_delay: int, save_settings: bool=False):\n",
    "        \"\"\"\n",
    "        Given a number of task retries and a retry_delay,\n",
    "        checks to make sure those values are valid\n",
    "        (ints greater than or equal to zero), and\n",
    "        optionally sets class variables to keep their\n",
    "        settings\n",
    "        \"\"\"\n",
    "        if isinstance(retries, int):\n",
    "            if cur_retries < 0:\n",
    "                raise ValueError(\"Number of task retries must be greater than or equal to zero\")\n",
    "            elif save_settings:\n",
    "                retries = cur_retries\n",
    "        elif retries is None:\n",
    "            cur_retries = retries\n",
    "        else:\n",
    "            raise TypeError(\"retries must be an int greater than or equal to zero\")\n",
    "\n",
    "        if isinstance(cur_retry_delay, int):\n",
    "            if cur_retry_delay < 0:\n",
    "                raise ValueError(\"Retry delay must be greater than or equal to zero\")\n",
    "            elif save_settings:\n",
    "                retry_delay = cur_retry_delay\n",
    "        elif cur_retry_delay is None:\n",
    "            cur_retry_delay = retry_delay\n",
    "        else:\n",
    "            raise TypeError(\"retry_delay must be an int greater than or equal to zero, representing the number of seconds to wait before retrying a task\")\n",
    "\n",
    "        return cur_retries, cur_retry_delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6360a9f7-bd2c-4f98-a1c6-52e9a602e62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def error_wrapper(func, args):\n",
    "        \"\"\"\n",
    "        This is the wrapper that is used when running individual tasks\n",
    "        It will always return a TaskResult!\n",
    "        \"\"\"\n",
    "        logger = get_logger()\n",
    "\n",
    "        for try_no in range(retries + 1):\n",
    "            try:\n",
    "                return TaskResult(0, \"Success\", args, func(*args))\n",
    "            except Exception as e:\n",
    "                if try_no < retries:\n",
    "                    logger.error(f\"Task failed with exception (retrying): {repr(e)}\")\n",
    "                    time.sleep(retry_delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.error(f\"Task failed with exception (giving up): {repr(e)}\")\n",
    "                    return TaskResult(1, repr(e), args, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "013bbff6-f3e2-45bc-ad7d-9867e9a38f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " def run_serial_tasks(name, func, input_list):\n",
    "        \"\"\"\n",
    "        Run tasks in serial (locally), given a function and list of inputs\n",
    "        This will always return a list of TaskResults!\n",
    "        \"\"\"\n",
    "        logger = get_logger()\n",
    "        logger.debug(f\"run_serial_tasks - input_list: {input_list}\")\n",
    "        return [error_wrapper(func, i) for i in input_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a3fb1-4222-4785-a6dc-002cd495dcea",
   "metadata": {},
   "source": [
    "TODO: fix chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0285d458-152f-4e47-a1c2-a7af9b36b459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunksize = 1\n",
    "def run_concurrent_tasks(name, func, input_list, force_sequential):\n",
    "        \"\"\"\n",
    "        Run tasks concurrently (locally), given a function a list of inputs\n",
    "        This will always return a list of TaskResults!\n",
    "        \"\"\"\n",
    "        pool_size = 1 if force_sequential else 10\n",
    "        with multiprocessing.Pool(pool_size) as pool:\n",
    "            results = pool.starmap(error_wrapper, [(func, i) for i in input_list], chunksize=chunksize)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb7c0786-7e93-483b-81cf-63bd925f37b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Optional' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_tasks\u001b[39m(func,\n\u001b[1;32m      2\u001b[0m                   input_list,\n\u001b[1;32m      3\u001b[0m                   allow_futures: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m----> 4\u001b[0m                   name: Optional[\u001b[38;5;28mstr\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      5\u001b[0m                   retries: Optional[\u001b[38;5;28mint\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[1;32m      6\u001b[0m                   retry_delay: Optional[\u001b[38;5;28mint\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m60\u001b[39m,\n\u001b[1;32m      7\u001b[0m                   force_sequential: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m                   force_serial: \u001b[38;5;28mbool\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m        Run a bunch of tasks, calling one of the above run_tasks functions\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m        This is the function that should be called most often from self.main()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m        It will return a ResultTuple of TaskResults\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         timestamp \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mtoday()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Optional' is not defined"
     ]
    }
   ],
   "source": [
    "def run_tasks(func,\n",
    "                  input_list,\n",
    "                  allow_futures: bool=True,\n",
    "                  name: Optional[str]=None,\n",
    "                  retries: Optional[int]=3,\n",
    "                  retry_delay: Optional[int]=60,\n",
    "                  force_sequential: bool=False,\n",
    "                  force_serial: bool=False):\n",
    "        \"\"\"\n",
    "        Run a bunch of tasks, calling one of the above run_tasks functions\n",
    "        This is the function that should be called most often from self.main()\n",
    "        It will return a ResultTuple of TaskResults\n",
    "        \"\"\"\n",
    "\n",
    "        timestamp = datetime.today()\n",
    "\n",
    "        if not callable(func):\n",
    "            raise TypeError(\"Function passed to run_tasks is not callable\")\n",
    "\n",
    "        # Save global retry settings, and override with current values\n",
    "        old_retries, old_retry_delay = retries, retry_delay\n",
    "        retries, retry_delay = init_retries(retries, retry_delay)\n",
    "\n",
    "        logger = get_logger()\n",
    "\n",
    "        if name is None:\n",
    "            try:\n",
    "                name = func.__name__\n",
    "            except AttributeError:\n",
    "                logger.warning(\"No name given for task run, and function does not have a name (multiple unnamed functions may result in log files being overwritten)\")\n",
    "                name = \"unnamed\"\n",
    "        elif not isinstance(name, str):\n",
    "            raise TypeError(\"Name of task run must be a string\")\n",
    "\n",
    "        if backend == \"serial\" or force_serial:\n",
    "            results = run_serial_tasks(name, func, input_list)\n",
    "        elif backend == \"concurrent\":\n",
    "            results = run_concurrent_tasks(name, func, input_list, force_sequential)\n",
    "        elif backend == \"prefect\":\n",
    "            results = run_prefect_tasks(name, func, input_list, force_sequential)\n",
    "        else:\n",
    "            raise ValueError(\"Requested backend not recognized. Have you called this Dataset's run function?\")\n",
    "\n",
    "        if len(results) == 0:\n",
    "            raise ValueError(f\"Task run {name} yielded no results. Did it receive any inputs?\")\n",
    "\n",
    "        success_count = sum(1 for r in results if r.status_code == 0)\n",
    "        error_count = len(results) - success_count\n",
    "        if error_count == 0:\n",
    "            logger.info(f\"Task run {name} completed with {success_count} successes and no errors\")\n",
    "        else:\n",
    "            logger.warning(f\"Task run {name} completed with {error_count} errors and {success_count} successes\")\n",
    "\n",
    "        # Restore global retry settings\n",
    "        retries, retry_delay = old_retries, old_retry_delay\n",
    "\n",
    "        return ResultTuple(results, name, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fb8d979-2e4f-48c2-9d70-19c72df6e550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_prefect_tasks(name, func, input_list, force_sequential):\n",
    "        \"\"\"\n",
    "        Run tasks using Prefect, using whichever task runner decided in self.run()\n",
    "        This will always return a list of TaskResults!\n",
    "        \"\"\"\n",
    "\n",
    "        from prefect import task\n",
    "        logger = get_logger()\n",
    "\n",
    "        task_wrapper = task(func, name=name, retries=retries, retry_delay_seconds=retry_delay, persist_result=True)\n",
    "\n",
    "        futures = []\n",
    "        for i in input_list:\n",
    "            w = [i[1] for i in futures] if force_sequential else None\n",
    "            futures.append((i, task_wrapper.submit(*i, wait_for=w, return_state=False)))\n",
    "\n",
    "        results = []\n",
    "\n",
    "\n",
    "        states = [(i[0], i[1].wait()) for i in futures]\n",
    "\n",
    "        while states:\n",
    "            for ix, (inputs, state) in enumerate(states):\n",
    "                if state.is_completed():\n",
    "                    # print('complete', ix, inputs)\n",
    "                    logger.info(f'complete - {ix} - {inputs}')\n",
    "\n",
    "                    results.append(TaskResult(0, \"Success\", inputs, state.result()))\n",
    "                elif state.is_failed() or state.is_crashed() or state.is_cancelled():\n",
    "                    # print('fail', ix, inputs)\n",
    "                    logger.info(f'fail - {ix} - {inputs}')\n",
    "\n",
    "                    try:\n",
    "                        msg = repr(state.result(raise_on_failure=True))\n",
    "                    except Exception as e:\n",
    "                        msg = f\"Unable to retrieve error message - {e}\"\n",
    "                    results.append(TaskResult(1, msg, inputs, None))\n",
    "                else:\n",
    "                    # print('not ready', ix, inputs)\n",
    "                    continue\n",
    "                _ = states.pop(ix)\n",
    "            time.sleep(5)\n",
    "\n",
    "\n",
    "        # for inputs, future in futures:\n",
    "        #     state = future.wait(60*60*2)\n",
    "        #     if state.is_completed():\n",
    "        #         results.append(TaskResult(0, \"Success\", inputs, state.result()))\n",
    "        #     elif state.is_failed() or state.is_crashed():\n",
    "        #         try:\n",
    "        #             msg = repr(state.result(raise_on_failure=False))\n",
    "        #         except:\n",
    "        #             msg = \"Unable to retrieve error message\"\n",
    "        #         results.append(TaskResult(1, msg, inputs, None))\n",
    "        #     else:\n",
    "        #         pass\n",
    "\n",
    "        # while futures:\n",
    "        #     for ix, (inputs, future) in enumerate(futures):\n",
    "        #         state = future.get_state()\n",
    "        #         # print(repr(state))\n",
    "        #         # print(repr(future))\n",
    "        #         if state.is_completed():\n",
    "        #             print('complete', ix, inputs)\n",
    "        #             results.append(TaskResult(0, \"Success\", inputs, future.result()))\n",
    "        #         elif state.is_failed() or state.is_crashed() or state.is_cancelled():\n",
    "        #             print('fail', ix, inputs)\n",
    "        #             try:\n",
    "        #                 msg = repr(future.result(raise_on_failure=True))\n",
    "        #             except Exception as e:\n",
    "        #                 msg = f\"Unable to retrieve error message - {e}\"\n",
    "        #             results.append(TaskResult(1, msg, inputs, None))\n",
    "        #         else:\n",
    "        #             # print('not ready', ix, inputs)\n",
    "        #             continue\n",
    "        #         _ = futures.pop(ix)\n",
    "        #         # future.release()\n",
    "        #     time.sleep(5)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce83c2-63c7-4da4-90fe-807589031630",
   "metadata": {},
   "source": [
    "build download list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a92a1e-a80c-4f9d-b86f-5fccd9fe5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_list = run_tasks(build_sensor_download_list, [[s] for s in sensors])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4356b2-d458-4d23-91bd-18c8c2182862",
   "metadata": {},
   "source": [
    "We have a list of lists (from each sensor), merge them into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9033c20-e013-46ea-b073-770016de3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = [i for i in chain(*raw_file_list.results())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c11e04-99c3-49b4-a3b6-75b0780d255c",
   "metadata": {},
   "source": [
    "Extract list of files to download from file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b99bc8-28f6-40e6-9162-8bd195655187",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_list = [i[1] for i in file_list if i[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220250-306d-41b0-a780-60c55869d576",
   "metadata": {},
   "source": [
    "Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f17d4-0cec-42d8-a4c7-13ab6c30f41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(download_list) > 0:\n",
    "    run_tasks(download, download_list).results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a4307f-00a3-4d58-b110-a4167668cd88",
   "metadata": {},
   "source": [
    "Make a list of all daily files, regardless of how the downloads went"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b8f0ef-17aa-474b-8b39-c5f879a46b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_files = [i[1][1] for i in file_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34836ab7-d932-4a0b-b7f3-e6214166862a",
   "metadata": {},
   "source": [
    "Build day dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09a043-1768-4160-8a05-3cd6f9a2f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_df = build_process_list(day_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f58d69-14df-4295-98f3-10c49337f9e7",
   "metadata": {},
   "source": [
    "build month dataframe\n",
    "Using pandas \"named aggregation\" to make ensure predictable column names in output.\n",
    "See bottom of this page:\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html\n",
    "see also https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#groupby-aggregate-named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495c2603-beb6-4307-883a-43e323433c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_df = day_df[[\"output_path\", \"year\", \"year_month\"]].groupby(\"year_month\", as_index=False).aggregate(\n",
    "            day_path_list = pd.NamedAgg(column=\"output_path\",   aggfunc=lambda x: tuple(x)),\n",
    "            count =         pd.NamedAgg(column=\"output_path\",   aggfunc=\"count\"),\n",
    "            year =          pd.NamedAgg(column=\"year\",          aggfunc=\"last\")\n",
    "        )\n",
    "minimum_days_in_month = 20\n",
    "month_df = month_df.loc[month_df[\"count\"] >= minimum_days_in_month]\n",
    "month_df[\"output_path\"] = month_df.apply(\n",
    "            lambda x: (output_dir / \"monthly/avhrr_ndvi_v5_{}.tif\".format(x[\"year_month\"])).as_posix(), axis=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd50920-e07a-4a55-bba8-028772d9674f",
   "metadata": {},
   "source": [
    "build year dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c6e05-113a-4e70-812e-9829e3e98ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_df = month_df[[\"output_path\", \"year\"]].groupby(\"year\", as_index=False).aggregate({\n",
    "            \"output_path\": [lambda x: tuple(x), \"count\"]\n",
    "        })\n",
    "year_df.columns = [\"year\", \"month_path_list\", \"count\"]\n",
    "\n",
    "year_df[\"output_path\"] = year_df[\"year\"].apply(\n",
    "            lambda x: (output_dir / f\"yearly/avhrr_ndvi_v5_{x}.tif\").as_posix()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e527bf-1907-4994-9eb3-37f63ef36816",
   "metadata": {},
   "source": [
    "Make _qlist arrays, which are handled by prep_xxx_data functions as lists of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e56b35-86b7-4f59-b029-14de8dffb44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_qlist = []\n",
    "for _, row in day_df.iterrows():\n",
    "    day_qlist.append([row[\"input_path\"], row[\"output_path\"]])\n",
    "\n",
    "month_qlist = []\n",
    "for _, row in month_df.iterrows():\n",
    "    month_qlist.append([row[\"year_month\"], row[\"day_path_list\"], row[\"output_path\"]])\n",
    "\n",
    "year_qlist = []\n",
    "for _, row in year_df.iterrows():\n",
    "    year_qlist.append([row[\"year\"], row[\"month_path_list\"], row[\"output_path\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec56c924-0400-4b95-89c6-d2408b4110b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"daily\" in build_list:\n",
    "    os.makedirs(output_dir / \"daily\", exist_ok=True)\n",
    "    run_tasks(process_daily_data, day_qlist)\n",
    "\n",
    "if \"monthly\" in build_list:\n",
    "    os.makedirs(output_dir / \"monthly\", exist_ok=True)\n",
    "    run_tasks(process_monthly_data, month_qlist)\n",
    "\n",
    "if \"yearly\" in build_list:\n",
    "    os.makedirs(output_dir / \"yearly\", exist_ok=True)\n",
    "    run_tasks(process_yearly_data, year_qlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbbef0c-3365-483e-8f96-064fe3deac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
