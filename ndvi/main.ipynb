{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c999dee-222a-4458-a531-34a780acb6d8",
   "metadata": {},
   "source": [
    "for use with NDVI product from LTDR raw dataset\n",
    "\n",
    "- Prepares list of all files\n",
    "- Builds list of day files to process\n",
    "- Processes day files\n",
    "- Builds list of day files to aggregate to months\n",
    "- Run month aggregation\n",
    "- Builds list of month files to aggregate to years\n",
    "- Run year aggregation\n",
    "\n",
    "example LTDR product file names (ndvi product code is AVH13C1)\n",
    "\n",
    "AVH13C1.A1981181.N07.004.2013227210959.hdf\n",
    "\n",
    "split file name by \".\"\n",
    "eg:\n",
    "\n",
    "full file name - \"AVH13C1.A1981181.N07.004.2013227210959.hdf\"\n",
    "\n",
    "0     product code        AVH13C1\n",
    "1     date of image       A1981181\n",
    "2     sensor code         N07\n",
    "3     misc                004\n",
    "4     processed date      2013227210959\n",
    "5     extension           hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0685b62f-5ed3-4737-a8fc-7b898b3c11e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import ssl\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from collections import OrderedDict\n",
    "from configparser import ConfigParser\n",
    "from typing import Any, Generator, List, Literal, Tuple, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6584b667-859e-4aa7-8bda-c7365d967b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.crs import CRS\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25f8fe1a-09c4-4bd5-9901-c7190e253dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal, osr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3279c6-ccd6-4dbf-b2e7-b88c1befacd6",
   "metadata": {},
   "source": [
    "!conda env create -f ./geo-datasets/env.yml\n",
    "!conda run -n geodata38"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c10465-9224-4f9c-8b86-a37e93d4245c",
   "metadata": {
    "tags": []
   },
   "source": [
    "!git clone --branch develop-k8s https://github.com/aiddata/geo-datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9357736-2280-4843-9898-22f51c5326be",
   "metadata": {
    "tags": []
   },
   "source": [
    "!pip install geo-datasets/global_scripts/data_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6036d3c8-05fd-4609-adcb-4d47fb9658ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_manager import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba39e8c4-4706-41e3-8105-1cc9164a5b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LTDR_NDVI(Dataset):\n",
    "\n",
    "    name = \"Long-term Data Record NDVI\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 token:str,\n",
    "                 years: List[Union[int, str]],\n",
    "                 raw_dir: Union[str, os.PathLike],\n",
    "                 output_dir: Union[str, os.PathLike],\n",
    "                 overwrite_download: bool,\n",
    "                 validate_download: bool,\n",
    "                 overwrite_processing: bool):\n",
    "\n",
    "        self.build_list = [\n",
    "            \"daily\",\n",
    "            \"monthly\",\n",
    "            \"yearly\"\n",
    "        ]\n",
    "\n",
    "        self.auth_headers = { \"Authorization\": f\"Bearer {token}\" }\n",
    "\n",
    "        self.years = [int(y) for y in years]\n",
    "\n",
    "        # TODO: warn if raw_dir already points to a directory named \"465\", it's probably one too deep\n",
    "        self.raw_dir = Path(raw_dir) / \"466\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "\n",
    "        self.overwrite_download = overwrite_download\n",
    "        self.validate_download = validate_download\n",
    "        self.overwrite_processing = overwrite_processing\n",
    "\n",
    "        self.dataset_url = \"https://ladsweb.modaps.eosdis.nasa.gov/api/v2/content/details/allData/466/\"\n",
    "\n",
    "        self.sensors = [\n",
    "            \"N07_AVH13C1\",\n",
    "            \"N09_AVH13C1\",\n",
    "            \"N11_AVH13C1\",\n",
    "            \"N14_AVH13C1\",\n",
    "            \"N16_AVH13C1\",\n",
    "            \"N18_AVH13C1\",\n",
    "            \"N19_AVH13C1\",\n",
    "        ]\n",
    "\n",
    "    def build_sensor_download_list(self, sensor: str):\n",
    "        logger = self.get_logger()\n",
    "\n",
    "        # generates dictionaries that represent each entry in a directory\n",
    "        def dir_contents(dir_url: str) -> List[dict]:\n",
    "            logger.debug(f\"Fetching {dir_url}\")\n",
    "            req_content = requests.get(dir_url).content\n",
    "            #TODO new add\n",
    "            # dir_url = \"https://ladsweb.modaps.eosdis.nasa.gov/api/v2/content/details/allData/466/\"\n",
    "            req_content = requests.get(dir_url).content\n",
    "            # print(dir_url)\n",
    "            description: dict = json.loads(requests.get(dir_url).content)\n",
    "            return description[\"content\"]\n",
    "\n",
    "        # validates md5 hash of a file\n",
    "        def validate(filepath: Union[str, os.PathLike], md5: str) -> bool:\n",
    "            with open(filepath, \"rb\") as chk:\n",
    "                data = chk.read()\n",
    "                return md5 == hashlib.md5(data).hexdigest()\n",
    "\n",
    "        # this is what we'll return\n",
    "        # list of tuples, each including:\n",
    "        #   1. a boolean \"does the file need to be downloaded?\"\n",
    "        #   2. another tuple: (url_of_download, dst_path_of_download)\n",
    "        download_list: List[Tuple[bool, Tuple[str, Type[Path]]]] = []\n",
    "\n",
    "        sensor_dir = str(urljoin(self.dataset_url, sensor))\n",
    "        # print(repr(sensor_dir))\n",
    "        # for each year the sensor collected data\n",
    "        for year_details in dir_contents(sensor_dir):\n",
    "            # is this a year we'd like data from?\n",
    "            if int(year_details[\"name\"]) in self.years:\n",
    "                # print(int(year_details[\"name\"]))\n",
    "                year_dir: str = \"/\".join([sensor_dir, year_details[\"name\"]])\n",
    "                print(year_dir)\n",
    "                # for each day the sensor collected data in this year\n",
    "                for day_details in dir_contents(year_dir):\n",
    "                    print(day_details)\n",
    "                    day_dir: str = \"/\".join([year_dir, day_details[\"name\"]])\n",
    "                    # for each file the sensor created for this day\n",
    "                    for file_detail in dir_contents(day_dir):\n",
    "                        day_download_url: str = file_detail[\"downloadsLink\"]\n",
    "                        dst = self.raw_dir / sensor / year_details[\"name\"] / day_details[\"name\"] / file_detail[\"name\"]\n",
    "                        # if file is already downloaded, and we aren't in overwrite mode\n",
    "                        if dst.exists() and not self.overwrite_download:\n",
    "                            if self.validate_download:\n",
    "                                if validate(dst, file_detail[\"md5sum\"]):\n",
    "                                    logger.info(f\"File validated: {dst.as_posix()}\")\n",
    "                                    download_list.append((False, (day_download_url, dst)))\n",
    "                                else:\n",
    "                                    logger.info(f\"File validation failed, queuing for download: {dst.as_posix()}\")\n",
    "                                    download_list.append((True, (day_download_url, dst)))\n",
    "                            else:\n",
    "                                logger.info(f\"File exists, skipping: {dst.as_posix()}\")\n",
    "                                download_list.append((False, (day_download_url, dst)))\n",
    "                        else:\n",
    "                            logger.info(f\"Queuing for download: {day_download_url}\")\n",
    "                            download_list.append((True, (day_download_url, dst)))\n",
    "        return download_list\n",
    "\n",
    "    def download(self, src_url: str, final_dst_path: Union[str, os.PathLike]) -> None:\n",
    "        logger = self.get_logger()\n",
    "        logger.info(f\"Downloading {str(final_dst_path)}...\")\n",
    "        with requests.get(src_url, headers=self.auth_headers, stream=True) as src:\n",
    "            src.raise_for_status()\n",
    "            with self.tmp_to_dst_file(final_dst_path) as dst_path:\n",
    "                # my addition\n",
    "                dst_parent = os.path.dirname(final_dst_path)\n",
    "                os.makedirs(dst_parent, exist_ok=True)\n",
    "\n",
    "                with open(dst_path, \"wb\") as dst:\n",
    "                    for chunk in src.iter_content(chunk_size=8192):\n",
    "                        dst.write(chunk)\n",
    "\n",
    "    def build_process_list(self, downloaded_files):\n",
    "\n",
    "        # filter options to accept/deny based on sensor, year\n",
    "        # all values must be strings\n",
    "        # do not enable/use both accept/deny for a given field\n",
    "\n",
    "        ops = {\n",
    "            \"use_sensor_accept\": False,\n",
    "            \"sensor_accept\": [],\n",
    "            \"use_sensor_deny\": False,\n",
    "            \"sensor_deny\": [],\n",
    "            \"use_year_accept\": True,\n",
    "            \"year_accept\": [\"2019\", \"2020\"],\n",
    "            \"use_year_deny\": False,\n",
    "            \"year_deny\": [\"2019\"]\n",
    "        }\n",
    "\n",
    "        df_dict_list = []\n",
    "\n",
    "        for input_path in downloaded_files:\n",
    "            items = input_path.stem.split(\".\")\n",
    "            year = items[1][1:5]\n",
    "            day = items[1][5:8]\n",
    "            sensor = items[2]\n",
    "            month = \"{0:02d}\".format(datetime.strptime(f\"{year}+{day}\", \"%Y+%j\").month)\n",
    "            output_path = self.output_dir / \"daily\" / f\"avhrr_ndvi_v5_{sensor}_{year}_{day}.tif\"\n",
    "            df_dict_list.append({\n",
    "                \"input_path\": input_path,\n",
    "                \"sensor\": sensor,\n",
    "                \"year\": year,\n",
    "                \"month\": month,\n",
    "                \"day\": day,\n",
    "                \"year_month\": year+\"_\"+month,\n",
    "                \"year_day\": year+\"_\"+day,\n",
    "                \"output_path\": output_path\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(df_dict_list).sort_values(by=[\"input_path\"])\n",
    "\n",
    "        # df = df.drop_duplicates(subset=\"year_day\", take_last=True)\n",
    "        sensors = sorted(list(set(df[\"sensor\"])))\n",
    "        years = sorted(list(set(df[\"year\"])))\n",
    "        filter_sensors = None\n",
    "        if ops['use_sensor_accept']:\n",
    "            filter_sensors = [i for i in sensors if i in ops['sensor_accept']]\n",
    "        elif ops['use_sensor_deny']:\n",
    "            filter_sensors = [i for i in sensors if i not in ops['sensor_deny']]\n",
    "        if filter_sensors:\n",
    "            df = df.loc[df[\"sensor\"].isin(filter_sensors)]\n",
    "        filter_years = None\n",
    "        if ops['use_year_accept']:\n",
    "            filter_years = [i for i in years if i in ops['year_accept']]\n",
    "        elif ops['use_year_deny']:\n",
    "            filter_years = [i for i in years if i not in ops['year_deny']]\n",
    "        if filter_years:\n",
    "            df = df.loc[df[\"year\"].isin(filter_years)]\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mask(qa_array, mask_vals):\n",
    "        qa_mask_vals = [abs(x - 15) for x in mask_vals]\n",
    "        mask_bin_array = [0] * 16\n",
    "        for x in qa_mask_vals:\n",
    "            mask_bin_array[x] = 1\n",
    "        mask_bin = int(\"\".join(map(str, mask_bin_array)), 2)\n",
    "\n",
    "        flag = lambda i: (i & 65535 & mask_bin) != 0\n",
    "\n",
    "        qa_mask = pd.DataFrame(qa_array).applymap(flag).to_numpy()\n",
    "        return qa_mask\n",
    "\n",
    "    def process_daily_data(self, src: Union[str, os.PathLike], output_path):\n",
    "            \"\"\"\n",
    "            Process input raster and create output in output directory\n",
    "\n",
    "            Unpack NDVI subdataset from a HDF container\n",
    "            Reproject to EPSG:4326\n",
    "            Set values <0 (other than nodata) to 0\n",
    "            Write to COG\n",
    "\n",
    "            Parts of code pulled from:\n",
    "\n",
    "            https://gis.stackexchange.com/questions/174017/extract-scientific-layers-from-modis-hdf-dataeset-using-python-gdal\n",
    "            https://gis.stackexchange.com/questions/42584/how-to-call-gdal-translate-from-python-code\n",
    "            https://stackoverflow.com/questions/10454316/how-to-project-and-resample-a-grid-to-match-another-grid-with-gdal-python/10538634#10538634\n",
    "            https://jgomezdans.github.io/gdal_notes/reprojection.html\n",
    "\n",
    "            Notes:\n",
    "\n",
    "            Rebuilding geotransform is not really necessary in this case but might\n",
    "            be useful for future data prep scripts that can use this as startng point.\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            logger = self.get_logger()\n",
    "\n",
    "            year = src.name.split(\".\")[1][1:5]\n",
    "            day = src.name.split(\".\")[1][5:8]\n",
    "            sensor = src.name.split(\".\")[2]\n",
    "\n",
    "            if output_path.exists() and not self.overwrite_processing:\n",
    "                logger.info(f\"Skipping day, already processed: {sensor} {year} {day}\")\n",
    "            else:\n",
    "                logger.info(f\"Processing day: {sensor} {year} {day}\")\n",
    "\n",
    "                # list of qa fields and bit numbers\n",
    "                # NOTE  link broken\n",
    "                # https://ltdr.modaps.eosdis.nasa.gov/ltdr/docs/AVHRR_LTDR_V5_Document.pdf\n",
    "\n",
    "                qa_bits = {\n",
    "                    15: \"Polar flag: latitude > 60deg (land) or > 50deg (ocean)\",\n",
    "                    14: \"BRDF-correction issues\",\n",
    "                    13: \"RHO3 value is invalid\",\n",
    "                    12: \"Channel 5 value is invalid\",\n",
    "                    11: \"Channel 4 value is invalid\",\n",
    "                    10: \"Channel 3 value is invalid\",\n",
    "                    9: \"Channel 2 (NIR) value is invalid\",\n",
    "                    8: \"Channel 1 (visible) value is invalid\",\n",
    "                    7: \"Channel 1-5 are invalid\",\n",
    "                    6: \"Pixel is at night (high solar zenith angle)\",\n",
    "                    5: \"Pixel is over dense dark vegetation\",\n",
    "                    4: \"Pixel is over sun glint\",\n",
    "                    3: \"Pixel is over water\",\n",
    "                    2: \"Pixel contains cloud shadow\",\n",
    "                    1: \"Pixel is cloudy\",\n",
    "                    0: \"Unused\"\n",
    "                }\n",
    "\n",
    "                # qa_mask_vals = [15, 9, 8, 6, 4, 3, 2, 1]\n",
    "                qa_mask_vals = [15, 9, 8, 1]\n",
    "\n",
    "                ndvi_gdal_path = f\"HDF4_EOS:EOS_GRID:\\\"{src.as_posix()}\\\":Grid:NDVI\"\n",
    "                # src_path = Path(src)\n",
    "                # ndvi_gdal_path = src_path\n",
    "                # print(self.output_dir)\n",
    "                # print(ndvi_gdal_path)\n",
    "                qa_gdal_path = f\"HDF4_EOS:EOS_GRID:\\\"{src.as_posix()}\\\":Grid:QA\"\n",
    "                # qa_gdal_path = src_path\n",
    "                \n",
    "                # my addition\n",
    "                # dst_parent_ndvi = os.path.dirname(ndvi_gdal_path)\n",
    "                # os.makedirs(dst_parent_ndvi, exist_ok=True)\n",
    "                # dst_parent_qa = os.path.dirname(qa_gdal_path)\n",
    "                # os.makedirs(dst_parent_qa, exist_ok=True)\n",
    "\n",
    "                # open data subdataset\n",
    "                \n",
    "                with rasterio.open(ndvi_gdal_path) as ndvi_src:\n",
    "                    ndvi_array = ndvi_src.read(1)\n",
    "\n",
    "                    # open quality assurance subdataset\n",
    "                    with rasterio.open(qa_gdal_path) as qa_src:\n",
    "                        qa_array = qa_src.read(1)\n",
    "\n",
    "                        # create mask array using our chosen mask values\n",
    "                        qa_mask = self.create_mask(qa_array, qa_mask_vals)\n",
    "\n",
    "                        # apply mask to dataset\n",
    "                        ndvi_array[qa_mask] = -9999\n",
    "\n",
    "                    ndvi_array[np.where((ndvi_array < 0) & (ndvi_array > -9999))] = 0\n",
    "                    ndvi_array[np.where(ndvi_array > 10000)] = 10000\n",
    "\n",
    "                    profile = {\n",
    "                        \"count\": 1,\n",
    "                        \"driver\": \"COG\",\n",
    "                        \"compress\": \"LZW\",\n",
    "                        \"dtype\": \"int16\",\n",
    "                        \"nodata\": -9999,\n",
    "                        \"height\": 3600,\n",
    "                        \"width\": 7200,\n",
    "                        \"crs\": CRS.from_epsg(4326),\n",
    "                        \"transform\": ndvi_src.transform,\n",
    "                    }\n",
    "\n",
    "                    with self.tmp_to_dst_file(output_path) as dst_path:\n",
    "                        with rasterio.open(dst_path, \"w\", **profile) as dst:\n",
    "                            # for some reason rasterio raises an exception if we don't specify that there is one index\n",
    "                            dst.write(ndvi_array, indexes=1)\n",
    "\n",
    "    def process_monthly_data(self, year_month, month_files, month_path):\n",
    "            logger = self.get_logger()\n",
    "            if os.path.exists(month_path) and not self.overwrite_processing:\n",
    "                logger.info(f\"Skipping month, already processed: {year_month}\")\n",
    "            else:\n",
    "                logger.info(f\"Processing month: {year_month}\")\n",
    "                data, meta = self.aggregate_rasters(file_list=month_files, method=\"max\")\n",
    "                self.write_raster(month_path, data, meta)\n",
    "\n",
    "    def process_yearly_data(self, year, year_files, year_path):\n",
    "        logger = self.get_logger()\n",
    "        if os.path.exists(year_path) and not self.overwrite_processing:\n",
    "            logger.info(f\"Skipping year, already processed: {year}\")\n",
    "        else:\n",
    "            logger.info(f\"Processing year: {year}\")\n",
    "            data, meta = self.aggregate_rasters(file_list=year_files, method=\"mean\")\n",
    "            self.write_raster(year_path, data, meta)\n",
    "\n",
    "    def aggregate_rasters(self, file_list, method=\"mean\"):\n",
    "            \"\"\"\n",
    "            Aggregate multiple rasters\n",
    "\n",
    "            Aggregates multiple rasters with same features (dimensions, transform,\n",
    "            pixel size, etc.) and creates single layer using aggregation method\n",
    "            specified.\n",
    "\n",
    "            Supported methods: mean (default), max, min, sum\n",
    "\n",
    "            Arguments\n",
    "                file_list (list): list of file paths for rasters to be aggregated\n",
    "                method (str): method used for aggregation\n",
    "\n",
    "            Return\n",
    "                result: rasterio Raster instance\n",
    "            \"\"\"\n",
    "            logger = self.get_logger()\n",
    "            store = None\n",
    "            for ix, file_path in enumerate(file_list):\n",
    "\n",
    "                try:\n",
    "                    raster = rasterio.open(file_path)\n",
    "                except:\n",
    "                    logger.error(f\"Could not include file in aggregation ({str(file_path)})\")\n",
    "                    continue\n",
    "\n",
    "                active = raster.read(masked=True)\n",
    "\n",
    "                if store is None:\n",
    "                    store = active.copy()\n",
    "\n",
    "                else:\n",
    "                    # make sure dimensions match\n",
    "                    if active.shape != store.shape:\n",
    "                        raise Exception(\"Dimensions of rasters do not match\")\n",
    "\n",
    "                    if method == \"max\":\n",
    "                        store = np.ma.array((store, active)).max(axis=0)\n",
    "\n",
    "                        # non masked array alternatives\n",
    "                        # store = np.maximum.reduce([store, active])\n",
    "                        # store = np.vstack([store, active]).max(axis=0)\n",
    "\n",
    "                    elif method == \"mean\":\n",
    "                        if ix == 1:\n",
    "                            weights = (~store.mask).astype(int)\n",
    "\n",
    "                        store = np.ma.average(np.ma.array((store, active)), axis=0, weights=[weights, (~active.mask).astype(int)])\n",
    "                        weights += (~active.mask).astype(int)\n",
    "\n",
    "                    elif method == \"min\":\n",
    "                        store = np.ma.array((store, active)).min(axis=0)\n",
    "\n",
    "                    elif method == \"sum\":\n",
    "                        store = np.ma.array((store, active)).sum(axis=0)\n",
    "\n",
    "                    else:\n",
    "                        raise Exception(\"Invalid method\")\n",
    "\n",
    "            store = store.filled(raster.nodata)\n",
    "            return store, raster.profile\n",
    "\n",
    "    def write_raster(self, path, data, meta):\n",
    "        logger = self.get_logger()\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        meta[\"dtype\"] = data.dtype\n",
    "        with self.tmp_to_dst_file(path) as write_path:\n",
    "            with rasterio.open(write_path, \"w\", **meta) as result:\n",
    "                try:\n",
    "                    result.write(data)\n",
    "                except:\n",
    "                    logger.exception(\"Error writing raster to {path}\")\n",
    "\n",
    "    def main(self):\n",
    "\n",
    "        # Build download list\n",
    "        raw_file_list = self.run_tasks(self.build_sensor_download_list, [[s] for s in self.sensors])\n",
    "\n",
    "        # We have a list of lists (from each sensor), merge them into one\n",
    "        file_list = [i for i in chain(*raw_file_list.results())]\n",
    "\n",
    "        # Extract list of files to download from file_list\n",
    "        download_list = [i[1] for i in file_list if i[0]]\n",
    "\n",
    "        # Download data\n",
    "        if len(download_list) > 0:\n",
    "            self.run_tasks(self.download, download_list).results()\n",
    "\n",
    "        # Make a list of all daily files, regardless of how the downloads went\n",
    "        day_files = [i[1][1] for i in file_list]\n",
    "\n",
    "        # Build day dataframe\n",
    "        day_df = self.build_process_list(day_files)\n",
    "\n",
    "        # build month dataframe\n",
    "\n",
    "        # Using pandas \"named aggregation\" to make ensure predictable column names in output.\n",
    "        # See bottom of this page:\n",
    "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html\n",
    "        # see also https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#groupby-aggregate-named\n",
    "        month_df = day_df[[\"output_path\", \"year\", \"year_month\"]].groupby(\"year_month\", as_index=False).aggregate(\n",
    "            day_path_list = pd.NamedAgg(column=\"output_path\",   aggfunc=lambda x: tuple(x)),\n",
    "            count =         pd.NamedAgg(column=\"output_path\",   aggfunc=\"count\"),\n",
    "            year =          pd.NamedAgg(column=\"year\",          aggfunc=\"last\")\n",
    "        )\n",
    "\n",
    "        minimum_days_in_month = 20\n",
    "\n",
    "        month_df = month_df.loc[month_df[\"count\"] >= minimum_days_in_month]\n",
    "\n",
    "        month_df[\"output_path\"] = month_df.apply(\n",
    "            lambda x: (self.output_dir / \"monthly/avhrr_ndvi_v5_{}.tif\".format(x[\"year_month\"])).as_posix(), axis=1\n",
    "        )\n",
    "\n",
    "        # build year dataframe\n",
    "        year_df = month_df[[\"output_path\", \"year\"]].groupby(\"year\", as_index=False).aggregate({\n",
    "            \"output_path\": [lambda x: tuple(x), \"count\"]\n",
    "        })\n",
    "        year_df.columns = [\"year\", \"month_path_list\", \"count\"]\n",
    "\n",
    "\n",
    "        year_df[\"output_path\"] = year_df[\"year\"].apply(\n",
    "            lambda x: (self.output_dir / f\"yearly/avhrr_ndvi_v5_{x}.tif\").as_posix()\n",
    "        )\n",
    "\n",
    "        # Make _qlist arrays, which are handled by prep_xxx_data functions as lists of tasks\n",
    "\n",
    "        day_qlist = []\n",
    "        for _, row in day_df.iterrows():\n",
    "            day_qlist.append([row[\"input_path\"], row[\"output_path\"]])\n",
    "\n",
    "        month_qlist = []\n",
    "        for _, row in month_df.iterrows():\n",
    "            month_qlist.append([row[\"year_month\"], row[\"day_path_list\"], row[\"output_path\"]])\n",
    "\n",
    "        year_qlist = []\n",
    "        for _, row in year_df.iterrows():\n",
    "            year_qlist.append([row[\"year\"], row[\"month_path_list\"], row[\"output_path\"]])\n",
    "\n",
    "        if \"daily\" in self.build_list:\n",
    "            os.makedirs(self.output_dir / \"daily\", exist_ok=True)\n",
    "            self.run_tasks(self.process_daily_data, day_qlist)\n",
    "\n",
    "        if \"monthly\" in self.build_list:\n",
    "            os.makedirs(self.output_dir / \"monthly\", exist_ok=True)\n",
    "            self.run_tasks(self.process_monthly_data, month_qlist)\n",
    "\n",
    "        if \"yearly\" in self.build_list:\n",
    "            os.makedirs(self.output_dir / \"yearly\", exist_ok=True)\n",
    "            self.run_tasks(self.process_yearly_data, year_qlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42d6c44e-1743-4297-913f-8802d4a519ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_config_dict(config_file=\"config.ini\"):\n",
    "    config = ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "    return {\n",
    "        \"token\": config[\"main\"][\"token\"],\n",
    "        \"years\": [int(y) for y in config[\"main\"][\"years\"].split(\", \")],\n",
    "        \"raw_dir\": Path(config[\"main\"][\"raw_dir\"]),\n",
    "        \"output_dir\": Path(config[\"main\"][\"output_dir\"]),\n",
    "        \"overwrite_download\": config[\"main\"].getboolean(\"overwrite_download\"),\n",
    "        \"validate_download\": config[\"main\"].getboolean(\"validate_download\"),\n",
    "        \"overwrite_processing\": config[\"main\"].getboolean(\"overwrite_processing\"),\n",
    "        \"backend\": config[\"run\"][\"backend\"],\n",
    "        \"task_runner\": config[\"run\"][\"task_runner\"],\n",
    "        \"run_parallel\": config[\"run\"].getboolean(\"run_parallel\"),\n",
    "        \"max_workers\": int(config[\"run\"][\"max_workers\"]),\n",
    "        \"log_dir\": Path(config[\"main\"][\"raw_dir\"]) / \"logs\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a9b8832-4cfc-474b-a596-80bfc853601f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 45] Operation not supported: '/home/jovyan'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/ltdr_ndvi/raw/logs/2024_02_16_14_06'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/ltdr_ndvi/raw/logs'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/ltdr_ndvi/raw'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jovyan/ltdr_ndvi'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m time_str \u001b[38;5;241m=\u001b[39m timestamp\u001b[38;5;241m.\u001b[39mstrftime(time_format_str)\n\u001b[1;32m      7\u001b[0m timestamp_log_dir \u001b[38;5;241m=\u001b[39m Path(log_dir) \u001b[38;5;241m/\u001b[39m time_str\n\u001b[0;32m----> 8\u001b[0m timestamp_log_dir\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Path.mkdir at line 1120 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:1120\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m   1119\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmkdir(mode, parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/pathlib.py:1116\u001b[0m, in \u001b[0;36mPath.mkdir\u001b[0;34m(self, mode, parents, exist_ok)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03mCreate a new directory at this given path.\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     os\u001b[38;5;241m.\u001b[39mmkdir(\u001b[38;5;28mself\u001b[39m, mode)\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[1;32m   1118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parents \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 45] Operation not supported: '/home/jovyan'"
     ]
    }
   ],
   "source": [
    "config_dict = get_config_dict()\n",
    "\n",
    "log_dir = config_dict[\"log_dir\"]\n",
    "timestamp = datetime.today()\n",
    "time_format_str: str=\"%Y_%m_%d_%H_%M\"\n",
    "time_str = timestamp.strftime(time_format_str)\n",
    "timestamp_log_dir = Path(log_dir) / time_str\n",
    "timestamp_log_dir.mkdir(parents=True, exist_ok=True)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d544efb-bdc2-4e7f-a7a7-fb3dcbc9ec95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_instance = LTDR_NDVI(config_dict[\"token\"], config_dict[\"years\"], config_dict[\"raw_dir\"], config_dict[\"output_dir\"], config_dict[\"overwrite_download\"], config_dict[\"validate_download\"], config_dict[\"overwrite_processing\"])\n",
    "\n",
    "class_instance.run(bypass_error_wrapper=False, backend=config_dict[\"backend\"], task_runner=config_dict[\"task_runner\"], run_parallel=config_dict[\"run_parallel\"], max_workers=config_dict[\"max_workers\"], log_dir=timestamp_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14058747-97d2-426a-87fc-5fdce4cc3144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
