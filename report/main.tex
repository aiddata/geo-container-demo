\documentclass[a4paper]{article}

\usepackage[margin=1in]{geometry} 
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{graphicx, color}
\usepackage{lipsum}
\usepackage{natbib}


% Research grant recipients must provide a summary report that is approximately two pages long, presenting the results of their funded research.  
% The report should include the status of the proposed work and note any significant changes from the initial proposal.
% The PI is expected to submit a summary report to the unit lead, Department Chair, Dean, Vice Provost for Research, and Provost.  

\title{Developing Data Pipelines for Research and Analysis}

\author{Seth Goodman$^1$\thanks{smgoodman@wm.edu} \and Jacob Hall$^1$ \and Cheyenne Hwang$^1$}

\date{
    $^1$AidData, Global Research Institute, William \& Mary \\ 
}

\begin{document}
\maketitle


\begin{abstract}

The growing demand for data and computational analysis in both research and educational initiatives stretches beyond disciplines such as computer or data science. As the efforts of faculty and students within research institutions become increasingly data-driven, traditional methods of utilizing critical resources such as high performance computing (HPC) clusters and disseminating the resulting work can present practical barriers to innovation. This work explores solutions to these issues through the development of a research use case which leverages open source software to build computational environments known as containers that can be deployed to run on computers ranging from individual laptops to HPC or cloud-based clusters with minimal modification. We demonstrate the use of containers to deploy a replicable and scalable environment for running data pipelines to acquire, prepare, and analyze satellite based measures of vegetation around Chinese financed mining projects at seven sites around the world prior to and after project completion. The resulting demonstration is now being expanded to support future research efforts and production data environments serving thousands of researchers around the world.

\noindent\textbf{Keywords:} containers, geospatial, data, replication, scalable
\end{abstract}


\section{Introduction}

Although the ability to reproduce and/or replicate findings is a central element of sound research, numerous disciplines currently face a "crisis of reproducibility"\citep{Baker2016}. In many fields, such as psychology and medicine, experiments can be practically difficult to implement in order to test reproducibility. Yet at a point in time where access to data and computational resources has never been higher, machine learning and data science research is also facing concerns over the ability to reproduce and replicate work \citep{Ding2020}.

Replicability is also a critical element of software development and deployment across industries, which have established robust systems, tools, and techniques for meeting their needs. The needs for replicability in the software product life cycle is multifaceted and can range from ensuring developers can design and test applications within identical environments (e.g., operating systems, dependency versions, configurations) to deploying multiple, identical copies of applications to servers across different time zones to provide optimal performance for users in different locations. Although the reasoning behind supporting replicability differs between industry and academia/research, similar approaches can be leveraged to share and replicate research as are used in industry to develop and deploy products.

Among tools that can be used to facilitate replication, containers are one of the most effective and flexible options available. Containers are a established technology for packing entire applications - including all the software, code, and environmental configurations needed to run them - into a portable format\citep{containers}. Containers are a powerful tool for replication on their own, but excel when used with additional tools that can be used to manage how and when containers run. One of the most common tools for orchestrating containers is Kubernetes - an industry standard open-source platform for managing workloads built using containers\citep{k8s}. Kubernetes' value outside of industry is becoming more apparent as research institutions shift from leveraging traditional HPC tools to containers and Kubernetes as a means of meeting the increasingly diverse computational demands of researchers and leveraging the benefits of industry supported advancements.

To illustrate the utility of containers for sharing replicable research, we developed a use case that integrates: A) use of a public repository (GitHub) for hosting code, container configuration file, documentation, and outputs, B) data pipelines for acquiring datasets needed to implement the research/analysis, C) data preparation and analysis scripts, and D) an accessible user interface within the container to run code within. These components can be adapted and extended to support a broad range of potential research applications, and can serve as a general template to facilitate research sharing and replication.

The use case developed focuses on evaluating the impact of Chinese financed mining projects on vegetation levels in surrounding areas. The analysis will incorporate data pipelines to acquire satellite imagery on vegetation levels and population as well as data on Chinese financed project locations. We will extract information around a subset of project sites associated with mining activities and assess trends in vegetation and population levels in the surrounding areas. 

The pipelines and analysis are contained with Notebook format (using Python code and Jupyter Hub notebooks) intended to provide an illustrative and adaptable template for a wide range of potential computational tasks incorporating large scale data such as satellite imagery that could be built and deployed using containers. The ability to develop research concepts on one machine, scale up for analysis on a cluster, then distribute to others for replication and review - without having to modify, rebuild, or troubleshoot code - is essential to accelerating the integration of data and computational analysis across fields.


\section{Results}
\lipsum[1]

GeoQuery\citep{Goodman2019}
AidData's Geospatial Global Chinese Development Finance dataset\citep{Goodman2024}
NASA's LTDR NDVI\citep{NASA2023}
WorldPop\citep{WorldPop2018}

public GitHub repository \footnote{url{https://github.com/aiddata/geo-container-demo}}

\section{Discussion}

Reference how this work served as the prototype for AidData's own scaled up data processing efforts using Kubernetes \footnote{See \url{https://github.com/aiddata/geo-datasets}}. Add more details on how it facilitated this, the amount of data we have processed, and how it has made it easier / supports GeoQuery (add in stats about GeoQuery usage, etc that this will help with).


\paragraph{Acknowledgements} 

We acknowledge the support of William \& Mary in funding this work through a W\&M Faculty Research Grant, and William \& Mary Research Computing for providing computational resources and/or technical support that have contributed to the results reported. 

\bibliographystyle{apalike}
\bibliography{refs.bib}	

\end{document}