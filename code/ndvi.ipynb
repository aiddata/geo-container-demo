{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c999dee-222a-4458-a531-34a780acb6d8",
   "metadata": {},
   "source": [
    "for use with NDVI product from LTDR raw dataset\n",
    "\n",
    "- Prepares list of all files\n",
    "- Builds list of day files to process\n",
    "- Processes day files\n",
    "- Builds list of day files to aggregate to months\n",
    "- Run month aggregation\n",
    "- Builds list of month files to aggregate to years\n",
    "- Run year aggregation\n",
    "\n",
    "example LTDR product file names (ndvi product code is AVH13C1)\n",
    "\n",
    "AVH13C1.A1981181.N07.004.2013227210959.hdf\n",
    "\n",
    "split file name by \".\"\n",
    "eg:\n",
    "\n",
    "full file name - \"AVH13C1.A1981181.N07.004.2013227210959.hdf\"\n",
    "\n",
    "0     product code        AVH13C1\n",
    "1     date of image       A1981181\n",
    "2     sensor code         N07\n",
    "3     misc                004\n",
    "4     processed date      2013227210959\n",
    "5     extension           hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0685b62f-5ed3-4737-a8fc-7b898b3c11e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import ssl\n",
    "import sys\n",
    "import json\n",
    "import hashlib\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "from itertools import chain\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "from collections import OrderedDict\n",
    "from configparser import ConfigParser\n",
    "from typing import Any, Generator, List, Literal, Tuple, Type, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6584b667-859e-4aa7-8bda-c7365d967b57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import rasterio\n",
    "import h5py\n",
    "from rasterio.crs import CRS\n",
    "from affine import Affine\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from osgeo import gdal, osr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6036d3c8-05fd-4609-adcb-4d47fb9658ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_manager import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba39e8c4-4706-41e3-8105-1cc9164a5b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LTDR_NDVI(Dataset):\n",
    "\n",
    "    name = \"Long-term Data Record NDVI\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 token:str,\n",
    "                 years: List[Union[int, str]],\n",
    "                 raw_dir: Union[str, os.PathLike],\n",
    "                 output_dir: Union[str, os.PathLike],\n",
    "                 overwrite_download: bool,\n",
    "                 validate_download: bool,\n",
    "                 overwrite_processing: bool):\n",
    "\n",
    "        self.build_list = [\n",
    "            \"daily\",\n",
    "            \"monthly\",\n",
    "            \"yearly\"\n",
    "        ]\n",
    "\n",
    "        self.auth_headers = { \"Authorization\": f\"Bearer {token}\" }\n",
    "\n",
    "        self.years = [int(y) for y in years]\n",
    "\n",
    "        # TODO: warn if raw_dir already points to a directory named \"465\", it's probably one too deep\n",
    "        self.raw_dir = Path(raw_dir) / \"466\"\n",
    "        self.output_dir = Path(output_dir)\n",
    "\n",
    "        self.overwrite_download = overwrite_download\n",
    "        self.validate_download = validate_download\n",
    "        self.overwrite_processing = overwrite_processing\n",
    "\n",
    "        self.dataset_url = \"https://ladsweb.modaps.eosdis.nasa.gov/api/v2/content/details/allData/466/\"\n",
    "\n",
    "        self.sensors = [\n",
    "            \"N07_AVH13C1\",\n",
    "            \"N09_AVH13C1\",\n",
    "            \"N11_AVH13C1\",\n",
    "            \"N14_AVH13C1\",\n",
    "            \"N16_AVH13C1\",\n",
    "            \"N18_AVH13C1\",\n",
    "            \"N19_AVH13C1\",\n",
    "        ]\n",
    "\n",
    "    def build_sensor_download_list(self, sensor: str):\n",
    "        logger = self.get_logger()\n",
    "\n",
    "        # generates dictionaries that represent each entry in a directory\n",
    "        def dir_contents(dir_url: str) -> List[dict]:\n",
    "            logger.debug(f\"Fetching {dir_url}\")\n",
    "            #req_content = requests.get(dir_url).content\n",
    "            #TODO new add\n",
    "            # dir_url = \"https://ladsweb.modaps.eosdis.nasa.gov/api/v2/content/details/allData/466/\"\n",
    "            i = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    # req_content = requests.get(dir_url).content\n",
    "                    description: dict = json.loads(requests.get(dir_url).content)\n",
    "                    return description[\"content\"]\n",
    "                except:\n",
    "                    i += 1\n",
    "                    if i > 4:\n",
    "                        raise\n",
    "            \n",
    "\n",
    "        # validates md5 hash of a file\n",
    "        def validate(filepath: Union[str, os.PathLike], md5: str) -> bool:\n",
    "            with open(filepath, \"rb\") as chk:\n",
    "                data = chk.read()\n",
    "                return md5 == hashlib.md5(data).hexdigest()\n",
    "\n",
    "        # this is what we'll return\n",
    "        # list of tuples, each including:\n",
    "        #   1. a boolean \"does the file need to be downloaded?\"\n",
    "        #   2. another tuple: (url_of_download, dst_path_of_download)\n",
    "        download_list: List[Tuple[bool, Tuple[str, Type[Path]]]] = []\n",
    "\n",
    "        sensor_dir = str(urljoin(self.dataset_url, sensor))\n",
    "        # print(repr(sensor_dir))\n",
    "        # for each year the sensor collected data\n",
    "        for year_details in dir_contents(sensor_dir):\n",
    "            # is this a year we'd like data from?\n",
    "            if int(year_details[\"name\"]) in self.years:\n",
    "                # print(int(year_details[\"name\"]))\n",
    "                year_dir: str = \"/\".join([sensor_dir, year_details[\"name\"]])\n",
    "                # print(\"~~:\", year_dir)\n",
    "                # for each day the sensor collected data in this year\n",
    "                for day_details in dir_contents(year_dir):\n",
    "                    # print(\"~~~:\", day_details)\n",
    "                    day_dir: str = \"/\".join([year_dir, day_details[\"name\"]])\n",
    "                    # for each file the sensor created for this day\n",
    "                    for file_detail in dir_contents(day_dir):\n",
    "                        day_download_url: str = file_detail[\"downloadsLink\"]\n",
    "                        dst = self.raw_dir / sensor / year_details[\"name\"] / day_details[\"name\"] / file_detail[\"name\"]\n",
    "                        # if file is already downloaded, and we aren't in overwrite mode\n",
    "                        if dst.exists() and not self.overwrite_download:\n",
    "                            if self.validate_download:\n",
    "                                if validate(dst, file_detail[\"md5sum\"]):\n",
    "                                    logger.info(f\"File validated: {dst.as_posix()}\")\n",
    "                                    download_list.append((False, (day_download_url, dst)))\n",
    "                                else:\n",
    "                                    logger.info(f\"File validation failed, queuing for download: {dst.as_posix()}\")\n",
    "                                    download_list.append((True, (day_download_url, dst)))\n",
    "                            else:\n",
    "                                logger.info(f\"File exists, skipping: {dst.as_posix()}\")\n",
    "                                download_list.append((False, (day_download_url, dst)))\n",
    "                        else:\n",
    "                            logger.info(f\"Queuing for download: {day_download_url}\")\n",
    "                            download_list.append((True, (day_download_url, dst)))\n",
    "        return download_list\n",
    "\n",
    "    def download(self, src_url: str, final_dst_path: Union[str, os.PathLike]) -> None:\n",
    "        logger = self.get_logger()\n",
    "        logger.info(f\"Downloading {str(final_dst_path)}...\")\n",
    "        with requests.get(src_url, headers=self.auth_headers, stream=True) as src:\n",
    "            src.raise_for_status()\n",
    "            with self.tmp_to_dst_file(final_dst_path) as dst_path:\n",
    "                # my addition\n",
    "                dst_parent = os.path.dirname(final_dst_path)\n",
    "                os.makedirs(dst_parent, exist_ok=True)\n",
    "\n",
    "                with open(dst_path, \"wb\") as dst:\n",
    "                    for chunk in src.iter_content(chunk_size=8192):\n",
    "                        dst.write(chunk)\n",
    "        logger.info(f\"Download complete: {str(final_dst_path)}\")\n",
    "\n",
    "    def build_process_list(self, downloaded_files):\n",
    "\n",
    "        # filter options to accept/deny based on sensor, year\n",
    "        # all values must be strings\n",
    "        # do not enable/use both accept/deny for a given field\n",
    "\n",
    "        ops = {\n",
    "            \"use_sensor_accept\": False,\n",
    "            \"sensor_accept\": [],\n",
    "            \"use_sensor_deny\": False,\n",
    "            \"sensor_deny\": [],\n",
    "            \"use_year_accept\": True,\n",
    "            \"year_accept\": [\"2019\", \"2020\"],\n",
    "            \"use_year_deny\": False,\n",
    "            \"year_deny\": [\"2019\"]\n",
    "        }\n",
    "\n",
    "        df_dict_list = []\n",
    "\n",
    "        for input_path in downloaded_files:\n",
    "            items = input_path.stem.split(\".\")\n",
    "            year = items[1][1:5]\n",
    "            day = items[1][5:8]\n",
    "            sensor = items[2]\n",
    "            month = \"{0:02d}\".format(datetime.strptime(f\"{year}+{day}\", \"%Y+%j\").month)\n",
    "            output_path = self.output_dir / \"daily\" / f\"avhrr_ndvi_v5_{sensor}_{year}_{day}.tif\"\n",
    "            df_dict_list.append({\n",
    "                \"input_path\": input_path,\n",
    "                \"sensor\": sensor,\n",
    "                \"year\": year,\n",
    "                \"month\": month,\n",
    "                \"day\": day,\n",
    "                \"year_month\": year+\"_\"+month,\n",
    "                \"year_day\": year+\"_\"+day,\n",
    "                \"output_path\": output_path\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(df_dict_list).sort_values(by=[\"input_path\"])\n",
    "\n",
    "        # df = df.drop_duplicates(subset=\"year_day\", take_last=True)\n",
    "        sensors = sorted(list(set(df[\"sensor\"])))\n",
    "        years = sorted(list(set(df[\"year\"])))\n",
    "        filter_sensors = None\n",
    "        if ops['use_sensor_accept']:\n",
    "            filter_sensors = [i for i in sensors if i in ops['sensor_accept']]\n",
    "        elif ops['use_sensor_deny']:\n",
    "            filter_sensors = [i for i in sensors if i not in ops['sensor_deny']]\n",
    "        if filter_sensors:\n",
    "            df = df.loc[df[\"sensor\"].isin(filter_sensors)]\n",
    "        filter_years = None\n",
    "        if ops['use_year_accept']:\n",
    "            filter_years = [i for i in years if i in ops['year_accept']]\n",
    "        elif ops['use_year_deny']:\n",
    "            filter_years = [i for i in years if i not in ops['year_deny']]\n",
    "        if filter_years:\n",
    "            df = df.loc[df[\"year\"].isin(filter_years)]\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mask(qa_array, mask_vals):\n",
    "        qa_mask_vals = [abs(x - 15) for x in mask_vals]\n",
    "        mask_bin_array = [0] * 16\n",
    "        for x in qa_mask_vals:\n",
    "            mask_bin_array[x] = 1\n",
    "        mask_bin = int(\"\".join(map(str, mask_bin_array)), 2)\n",
    "\n",
    "        flag = lambda i: (i & 65535 & mask_bin) != 0\n",
    "\n",
    "        qa_mask = pd.DataFrame(qa_array).applymap(flag).to_numpy()\n",
    "        return qa_mask\n",
    "\n",
    "    def process_daily_data(self, src: Union[str, os.PathLike], output_path):\n",
    "            \"\"\"\n",
    "            Process input raster and create output in output directory\n",
    "\n",
    "            Unpack NDVI subdataset from a HDF container\n",
    "            Reproject to EPSG:4326\n",
    "            Set values <0 (other than nodata) to 0\n",
    "            Write to COG\n",
    "\n",
    "            Parts of code pulled from:\n",
    "\n",
    "            https://gis.stackexchange.com/questions/174017/extract-scientific-layers-from-modis-hdf-dataeset-using-python-gdal\n",
    "            https://gis.stackexchange.com/questions/42584/how-to-call-gdal-translate-from-python-code\n",
    "            https://stackoverflow.com/questions/10454316/how-to-project-and-resample-a-grid-to-match-another-grid-with-gdal-python/10538634#10538634\n",
    "            https://jgomezdans.github.io/gdal_notes/reprojection.html\n",
    "\n",
    "            Notes:\n",
    "\n",
    "            Rebuilding geotransform is not really necessary in this case but might\n",
    "            be useful for future data prep scripts that can use this as startng point.\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "            logger = self.get_logger()\n",
    "\n",
    "            year = src.name.split(\".\")[1][1:5]\n",
    "            day = src.name.split(\".\")[1][5:8]\n",
    "            sensor = src.name.split(\".\")[2]\n",
    "\n",
    "            if output_path.exists() and not self.overwrite_processing:\n",
    "                logger.info(f\"Skipping day, already processed: {sensor} {year} {day}\")\n",
    "            else:\n",
    "                logger.info(f\"Processing day: {sensor} {year} {day}\")\n",
    "\n",
    "                # list of qa fields and bit numbers\n",
    "                # NOTE  link broken\n",
    "                # https://ltdr.modaps.eosdis.nasa.gov/ltdr/docs/AVHRR_LTDR_V5_Document.pdf\n",
    "\n",
    "                qa_bits = {\n",
    "                    15: \"Polar flag: latitude > 60deg (land) or > 50deg (ocean)\",\n",
    "                    14: \"BRDF-correction issues\",\n",
    "                    13: \"RHO3 value is invalid\",\n",
    "                    12: \"Channel 5 value is invalid\",\n",
    "                    11: \"Channel 4 value is invalid\",\n",
    "                    10: \"Channel 3 value is invalid\",\n",
    "                    9: \"Channel 2 (NIR) value is invalid\",\n",
    "                    8: \"Channel 1 (visible) value is invalid\",\n",
    "                    7: \"Channel 1-5 are invalid\",\n",
    "                    6: \"Pixel is at night (high solar zenith angle)\",\n",
    "                    5: \"Pixel is over dense dark vegetation\",\n",
    "                    4: \"Pixel is over sun glint\",\n",
    "                    3: \"Pixel is over water\",\n",
    "                    2: \"Pixel contains cloud shadow\",\n",
    "                    1: \"Pixel is cloudy\",\n",
    "                    0: \"Unused\"\n",
    "                }\n",
    "\n",
    "                # qa_mask_vals = [15, 9, 8, 6, 4, 3, 2, 1]\n",
    "                qa_mask_vals = [15, 9, 8, 1]\n",
    "                \n",
    "                nc_data_path = src.as_posix()\n",
    "                \n",
    "                nc_data = h5py.File(nc_data_path, 'r')\n",
    "                \n",
    "                ndvi_array = nc_data[\"NDVI\"][:]\n",
    "                qa_array = nc_data[\"QA\"][:]\n",
    "                lat = nc_data[\"latitude\"]\n",
    "                lon = nc_data[\"longitude\"]\n",
    "                \n",
    "                ndvi_transform = Affine(0.05, 0, min(lon), 0, -0.05, max(lat))\n",
    "                \n",
    "                profile = {\n",
    "                    \"count\": 1,\n",
    "                    \"driver\": \"COG\",\n",
    "                    \"compress\": \"LZW\",\n",
    "                    \"dtype\": \"int16\",\n",
    "                    \"nodata\": -9999,\n",
    "                    \"height\": 3600,\n",
    "                    \"width\": 7200,\n",
    "                    \"crs\": CRS.from_epsg(4326),\n",
    "                    \"transform\": ndvi_transform,\n",
    "                }\n",
    "\n",
    "\n",
    "                # create mask array using our chosen mask values\n",
    "                qa_mask = self.create_mask(qa_array, qa_mask_vals)\n",
    "\n",
    "                # apply mask to dataset\n",
    "                ndvi_array[qa_mask] = -9999\n",
    "\n",
    "                ndvi_array[np.where((ndvi_array < 0) & (ndvi_array > -9999))] = 0\n",
    "                ndvi_array[np.where(ndvi_array > 10000)] = 10000\n",
    "\n",
    "                with self.tmp_to_dst_file(output_path) as dst_path:\n",
    "                    with rasterio.open(dst_path, \"w\", **profile) as dst:\n",
    "                        # for some reason rasterio raises an exception if we don't specify that there is one index\n",
    "                        dst.write(ndvi_array, indexes=1)\n",
    "\n",
    "    def process_monthly_data(self, year_month, month_files, month_path):\n",
    "            logger = self.get_logger()\n",
    "            if os.path.exists(month_path) and not self.overwrite_processing:\n",
    "                logger.info(f\"Skipping month, already processed: {year_month}\")\n",
    "            else:\n",
    "                logger.info(f\"Processing month: {year_month}\")\n",
    "                data, meta = self.aggregate_rasters(file_list=month_files, method=\"max\")\n",
    "                self.write_raster(month_path, data, meta)\n",
    "\n",
    "    def process_yearly_data(self, year, year_files, year_path):\n",
    "        logger = self.get_logger()\n",
    "        if os.path.exists(year_path) and not self.overwrite_processing:\n",
    "            logger.info(f\"Skipping year, already processed: {year}\")\n",
    "        else:\n",
    "            logger.info(f\"Processing year: {year}\")\n",
    "            data, meta = self.aggregate_rasters(file_list=year_files, method=\"mean\")\n",
    "            self.write_raster(year_path, data, meta)\n",
    "\n",
    "    def aggregate_rasters(self, file_list, method=\"mean\"):\n",
    "            \"\"\"\n",
    "            Aggregate multiple rasters\n",
    "\n",
    "            Aggregates multiple rasters with same features (dimensions, transform,\n",
    "            pixel size, etc.) and creates single layer using aggregation method\n",
    "            specified.\n",
    "\n",
    "            Supported methods: mean (default), max, min, sum\n",
    "\n",
    "            Arguments\n",
    "                file_list (list): list of file paths for rasters to be aggregated\n",
    "                method (str): method used for aggregation\n",
    "\n",
    "            Return\n",
    "                result: rasterio Raster instance\n",
    "            \"\"\"\n",
    "            logger = self.get_logger()\n",
    "            store = None\n",
    "            for ix, file_path in enumerate(file_list):\n",
    "\n",
    "                try:\n",
    "                    raster = rasterio.open(file_path)\n",
    "                except:\n",
    "                    logger.error(f\"Could not include file in aggregation ({str(file_path)})\")\n",
    "                    continue\n",
    "\n",
    "                active = raster.read(masked=True)\n",
    "\n",
    "                if store is None:\n",
    "                    store = active.copy()\n",
    "\n",
    "                else:\n",
    "                    # make sure dimensions match\n",
    "                    if active.shape != store.shape:\n",
    "                        raise Exception(\"Dimensions of rasters do not match\")\n",
    "\n",
    "                    if method == \"max\":\n",
    "                        store = np.ma.array((store, active)).max(axis=0)\n",
    "\n",
    "                        # non masked array alternatives\n",
    "                        # store = np.maximum.reduce([store, active])\n",
    "                        # store = np.vstack([store, active]).max(axis=0)\n",
    "\n",
    "                    elif method == \"mean\":\n",
    "                        if ix == 1:\n",
    "                            weights = (~store.mask).astype(int)\n",
    "\n",
    "                        store = np.ma.average(np.ma.array((store, active)), axis=0, weights=[weights, (~active.mask).astype(int)])\n",
    "                        weights += (~active.mask).astype(int)\n",
    "\n",
    "                    elif method == \"min\":\n",
    "                        store = np.ma.array((store, active)).min(axis=0)\n",
    "\n",
    "                    elif method == \"sum\":\n",
    "                        store = np.ma.array((store, active)).sum(axis=0)\n",
    "\n",
    "                    else:\n",
    "                        raise Exception(\"Invalid method\")\n",
    "\n",
    "            store = store.filled(raster.nodata)\n",
    "            return store, raster.profile\n",
    "\n",
    "    def write_raster(self, path, data, meta):\n",
    "        logger = self.get_logger()\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        meta[\"dtype\"] = data.dtype\n",
    "        with self.tmp_to_dst_file(path) as write_path:\n",
    "            with rasterio.open(write_path, \"w\", **meta) as result:\n",
    "                try:\n",
    "                    result.write(data)\n",
    "                except:\n",
    "                    logger.exception(\"Error writing raster to {path}\")\n",
    "\n",
    "    def main(self):\n",
    "\n",
    "        # Build download list\n",
    "        raw_file_list = self.run_tasks(self.build_sensor_download_list, [[s] for s in self.sensors])\n",
    "\n",
    "        # We have a list of lists (from each sensor), merge them into one\n",
    "        file_list = [i for i in chain(*raw_file_list.results())]\n",
    "\n",
    "        # Extract list of files to download from file_list\n",
    "        download_list = [i[1] for i in file_list if i[0]]\n",
    "        # download_list = download_list[:5]\n",
    "        for i in download_list:\n",
    "            print(i)\n",
    "\n",
    "        # Download data\n",
    "        if len(download_list) > 0:\n",
    "            self.run_tasks(self.download, download_list, force_serial=True).results()\n",
    "\n",
    "        # Make a list of all daily files, regardless of how the downloads went\n",
    "        day_files = [i[1][1] for i in file_list]\n",
    "\n",
    "        # Build day dataframe\n",
    "        day_df = self.build_process_list(day_files)\n",
    "\n",
    "        # build month dataframe\n",
    "\n",
    "        # Using pandas \"named aggregation\" to make ensure predictable column names in output.\n",
    "        # See bottom of this page:\n",
    "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.DataFrameGroupBy.aggregate.html\n",
    "        # see also https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#groupby-aggregate-named\n",
    "        month_df = day_df[[\"output_path\", \"year\", \"year_month\"]].groupby(\"year_month\", as_index=False).aggregate(\n",
    "            day_path_list = pd.NamedAgg(column=\"output_path\",   aggfunc=lambda x: tuple(x)),\n",
    "            count =         pd.NamedAgg(column=\"output_path\",   aggfunc=\"count\"),\n",
    "            year =          pd.NamedAgg(column=\"year\",          aggfunc=\"last\")\n",
    "        )\n",
    "\n",
    "        minimum_days_in_month = 20\n",
    "\n",
    "        month_df = month_df.loc[month_df[\"count\"] >= minimum_days_in_month]\n",
    "\n",
    "        month_df[\"output_path\"] = month_df.apply(\n",
    "            lambda x: (self.output_dir / \"monthly/avhrr_ndvi_v5_{}.tif\".format(x[\"year_month\"])).as_posix(), axis=1\n",
    "        )\n",
    "\n",
    "        # build year dataframe\n",
    "        year_df = month_df[[\"output_path\", \"year\"]].groupby(\"year\", as_index=False).aggregate({\n",
    "            \"output_path\": [lambda x: tuple(x), \"count\"]\n",
    "        })\n",
    "        year_df.columns = [\"year\", \"month_path_list\", \"count\"]\n",
    "\n",
    "\n",
    "        year_df[\"output_path\"] = year_df[\"year\"].apply(\n",
    "            lambda x: (self.output_dir / f\"yearly/avhrr_ndvi_v5_{x}.tif\").as_posix()\n",
    "        )\n",
    "\n",
    "        # Make _qlist arrays, which are handled by prep_xxx_data functions as lists of tasks\n",
    "\n",
    "        day_qlist = []\n",
    "        for _, row in day_df.iterrows():\n",
    "            day_qlist.append([row[\"input_path\"], row[\"output_path\"]])\n",
    "\n",
    "        month_qlist = []\n",
    "        for _, row in month_df.iterrows():\n",
    "            month_qlist.append([row[\"year_month\"], row[\"day_path_list\"], row[\"output_path\"]])\n",
    "\n",
    "        year_qlist = []\n",
    "        for _, row in year_df.iterrows():\n",
    "            year_qlist.append([row[\"year\"], row[\"month_path_list\"], row[\"output_path\"]])\n",
    "\n",
    "        if \"daily\" in self.build_list:\n",
    "            os.makedirs(self.output_dir / \"daily\", exist_ok=True)\n",
    "            self.run_tasks(self.process_daily_data, day_qlist)\n",
    "\n",
    "        if \"monthly\" in self.build_list:\n",
    "            os.makedirs(self.output_dir / \"monthly\", exist_ok=True)\n",
    "            self.run_tasks(self.process_monthly_data, month_qlist)\n",
    "\n",
    "        if \"yearly\" in self.build_list:\n",
    "            os.makedirs(self.output_dir / \"yearly\", exist_ok=True)\n",
    "            self.run_tasks(self.process_yearly_data, year_qlist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d6c44e-1743-4297-913f-8802d4a519ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_config_dict(config_file=\"config.ini\"):\n",
    "    config = ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "    return {\n",
    "        \"token\": config[\"main\"][\"token\"],\n",
    "        \"years\": [int(y) for y in config[\"main\"][\"years\"].split(\", \")],\n",
    "        \"raw_dir\": Path(config[\"main\"][\"raw_dir\"]),\n",
    "        \"output_dir\": Path(config[\"main\"][\"output_dir\"]),\n",
    "        \"overwrite_download\": config[\"main\"].getboolean(\"overwrite_download\"),\n",
    "        \"validate_download\": config[\"main\"].getboolean(\"validate_download\"),\n",
    "        \"overwrite_processing\": config[\"main\"].getboolean(\"overwrite_processing\"),\n",
    "        \"backend\": config[\"run\"][\"backend\"],\n",
    "        \"task_runner\": config[\"run\"][\"task_runner\"],\n",
    "        \"run_parallel\": config[\"run\"].getboolean(\"run_parallel\"),\n",
    "        \"max_workers\": int(config[\"run\"][\"max_workers\"]),\n",
    "        \"log_dir\": Path(config[\"main\"][\"raw_dir\"]) / \"logs\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a9b8832-4cfc-474b-a596-80bfc853601f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/data/ndvi/raw/logs/2024_08_07_17_56\n"
     ]
    }
   ],
   "source": [
    "config_dict = get_config_dict()\n",
    "\n",
    "log_dir = config_dict[\"log_dir\"]\n",
    "timestamp = datetime.today()\n",
    "time_format_str: str=\"%Y_%m_%d_%H_%M\"\n",
    "time_str = timestamp.strftime(time_format_str)\n",
    "timestamp_log_dir = Path(log_dir) / time_str\n",
    "timestamp_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(timestamp_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79b0d812-9a93-4e5a-91b7-20baa016afcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_instance = LTDR_NDVI(config_dict[\"token\"], config_dict[\"years\"], config_dict[\"raw_dir\"], config_dict[\"output_dir\"], config_dict[\"overwrite_download\"], config_dict[\"validate_download\"], config_dict[\"overwrite_processing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6dfcb37-58f7-4ccc-9642-9e50efff416d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your conda environment is base instead of the expected geodata38\n",
      "No $TMPDIR environment variable found!\n",
      "File exists, skipping: /home/jovyan/data/ndvi/raw/466/N16_AVH13C1/2001/001/N16_AVH13C1.A2001001.006.2022287122402.nc\n",
      "File exists, skipping: /home/jovyan/data/ndvi/raw/466/N16_AVH13C1/2001/002/N16_AVH13C1.A2001002.006.2022287122406.nc\n",
      "File exists, skipping: /home/jovyan/data/ndvi/raw/466/N16_AVH13C1/2001/003/N16_AVH13C1.A2001003.006.2022287122422.nc\n",
      "File exists, skipping: /home/jovyan/data/ndvi/raw/466/N16_AVH13C1/2001/004/N16_AVH13C1.A2001004.006.2022287122531.nc\n",
      "File exists, skipping: /home/jovyan/data/ndvi/raw/466/N16_AVH13C1/2001/005/N16_AVH13C1.A2001005.006.2022287122716.nc\n",
      "File exists, skipping: /home/jovyan/data/ndvi/raw/466/N16_AVH13C1/2001/006/N16_AVH13C1.A2001006.006.2022287122841.nc\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclass_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbypass_error_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_runner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtask_runner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_parallel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_parallel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp_log_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/data_manager/dataset.py:657\u001b[0m, in \u001b[0;36mDataset.run\u001b[0;34m(self, backend, task_runner, run_parallel, max_workers, threads_per_worker, chunksize, log_dir, logger_level, retries, retry_delay, conda_env, bypass_error_wrapper, **kwargs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserial\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 657\u001b[0m     \u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBackend \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not recognized.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/data_manager/dataset.py:534\u001b[0m, in \u001b[0;36mDataset.run.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    526\u001b[0m \u001b[38;5;124;03mRun a dataset\u001b[39;00m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;124;03mInitializes class variables and chosen backend\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03mThis is how Datasets should usually be run\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03mEventually calls _check_env_and_run(), starting dataset (see below)\u001b[39;00m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;66;03m# no matter what happens, this is our ticket to run the actual dataset\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;66;03m# every backend calls this after initializing\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m launch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_env_and_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconda_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_retries(retries, retry_delay, save_settings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_dir \u001b[38;5;241m=\u001b[39m Path(log_dir)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/data_manager/dataset.py:506\u001b[0m, in \u001b[0;36mDataset._check_env_and_run\u001b[0;34m(self, correct_env)\u001b[0m\n\u001b[1;32m    501\u001b[0m             logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    502\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$TMPDIR in /local, deployments won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be accessible to compute nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m             )\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# run the dataset (self.main() should be defined in child class instance)\u001b[39;00m\n\u001b[0;32m--> 506\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 390\u001b[0m, in \u001b[0;36mLTDR_NDVI.main\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m \n\u001b[1;32m    389\u001b[0m     \u001b[38;5;66;03m# Build download list\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     raw_file_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_sensor_download_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msensors\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# We have a list of lists (from each sensor), merge them into one\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     file_list \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;241m*\u001b[39mraw_file_list\u001b[38;5;241m.\u001b[39mresults())]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/data_manager/dataset.py:315\u001b[0m, in \u001b[0;36mDataset.run_tasks\u001b[0;34m(self, func, input_list, allow_futures, name, retries, retry_delay, force_sequential, force_serial)\u001b[0m\n\u001b[1;32m    313\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_serial_tasks(name, func, input_list)\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconcurrent\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 315\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_concurrent_tasks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_sequential\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefect\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    319\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_prefect_tasks(name, func, input_list, force_sequential)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/data_manager/dataset.py:161\u001b[0m, in \u001b[0;36mDataset.run_concurrent_tasks\u001b[0;34m(self, name, func, input_list, force_sequential)\u001b[0m\n\u001b[1;32m    159\u001b[0m pool_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m force_sequential \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(pool_size) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[0;32m--> 161\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstarmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:375\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstarmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    370\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;124;03m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;124;03m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:768\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 768\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mready():\n\u001b[1;32m    770\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:629\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    627\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 629\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class_instance.run(bypass_error_wrapper=False, backend=config_dict[\"backend\"], task_runner=config_dict[\"task_runner\"], run_parallel=config_dict[\"run_parallel\"], max_workers=config_dict[\"max_workers\"], log_dir=timestamp_log_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
